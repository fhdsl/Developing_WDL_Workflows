[["index.html", "Developing WDL Workflows About this Course 0.1 Target Audience 0.2 Why WDL? 0.3 Curriculum", " Developing WDL Workflows January, 2025 About this Course “Developing WDL Workflows” shows a bioinformatics workflow developer how to strategically develop and scale up a WDL workflow that is iterative, reproducible, and efficient in terms of time and resource used. This guide is flexible regardless of where the data is, what computing resources are being used, and what software is being used. 0.1 Target Audience The course is intended for first time developers of the WDL workflow language, who wants to iteratively develop a WDL bioinformatics workflow. In order to use this guide the audience should be able to comprehend introductory WDL syntax, and should be able to run a WDL workflow on a computing engine of their choice, such as Cromwell, miniwdl, or a cloud computing environment such as Terra, AnVIL, or Dockstore. 0.1.1 Relevant Resources If you are new to WDL, OpenWDL Docs is an excellent WDL companion resource to help you get started and is platform agnostic. OpenWDL Docs focuses on the basic grammar of WDL as well as providing excellent cookbook recipes of common WDL workflow structures. In this guide we will reference these basic grammar structures and common workflow cookbook recipes. 0.2 Why WDL? You may have encountered other workflow tools, such as Snakemake or Nextflow, and those are highly capable. Why learn a brand new workflow language? Let’s review some WDL Pros and Cons: 0.2.1 WDL Pros WDL has some really helpful advantages compared to other frameworks: Portability. WDL can run on nearly any system, whether it be your local computer, or on an HPC cluster, or on the Cloud, with platforms such as DNAnexus. In fact, a lot of developers will prototype a WDL workflow on their own local computer before moving it to the cloud. Reproducibility. Ever have the headache of having to reproduce the exact package versions to get your workflow to work again? If you use Docker containers to specify your software environment, you do not have to worry about this headache. A workflow will run identically locally, on HPC, or the cloud. Sharing. A WDL workflow is much easier to share with colleagues and is a good way to get credit for work you do everyday. If you spent time building it, why not share it? WDL is also an open standard and supported by a number of software tools. Running and Making WDL workflows is a transferable skill. Genomics and Pharma companies rely on WDL workflows to process thousands of FASTA/VCF files for their studies. They need more experts. It makes you more hireable within both Academia and Industry. 0.2.2 WDL Cons Of course, nothing is free. WDL does require you to understand the basic concepts and terminologies including: Basics of Docker Understanding the WDL framework Converting your bash scripts into WDL tasks and workflows 0.3 Curriculum The course covers the following: How to write an effective WDL task Link multiple WDL tasks together in a workflow Organize variables via structs, scale multiple samples via Arrays Reuse repeated tasks via task aliasing Configure settings for the execution engine "],["introduction-to-wdl.html", "Chapter 1 Introduction to WDL 1.1 Introduction 1.2 Review of basic WDL syntax 1.3 Using JSONs to control workflow inputs 1.4 Running WDL via a computing engine", " Chapter 1 Introduction to WDL 1.1 Introduction Welcome to building your first WDL workflow! This guide will help you strategically develop and scale up a WDL workflow that is iterative, reproducible, and efficient in terms of time and resource used. To make sure that we are on the same page, this guide assumes that you are able to run a WDL on a computing engine of your choice, such as Cromwell, miniwdl, or a cloud computing environment such as Terra, AnVIL, or Dockstore. This guide also assumes that you have a beginner’s understanding of the WDL syntax, and we will link out to additional resources to fill in the knowledge gap as needed! If you have never seen the WDL language in action, a great place to start is OpenWDL docs – it teaches you the basic syntax and showcases WDL features via concrete examples. 1.2 Review of basic WDL syntax We will do some review of the WDL syntax. A WDL workflow consists of at least one task. version 1.0 task do_something { command &lt;&lt;&lt; exit 0 &gt;&gt;&gt; } workflow my_workflow { call do_something } A workflow, and the tasks it calls, generally has inputs. version 1.0 task do_something { input { File fastq } command &lt;&lt;&lt; exit 0 &gt;&gt;&gt; } workflow my_workflow { input { File fq } call do_something { input: fastq = fq } } The input fq is defined to be a File variable type. WDL supports various variable types, such as String, Integer, Float, and Boolean. For more information on types in WDL, we recommend OpenWDL’s documentation on variable types. To access a task-level input variable in a task’s command section, it is usually referenced using ~{input_name} notation, such as (~{fastq}) in the example below. To access a workflow-level variable in a workflow, it is referenced just by its name without any special notation, such as fq in the example below. To access a workflow-level variable in a task, it must be passed into the task as an input. version 1.0 task do_something { input { File fastq String basename_of_fq } command &lt;&lt;&lt; echo &quot;First ten lines of ~{basename_of_fq}: &quot; head ~{fastq} ## example of referring to task-level input &gt;&gt;&gt; } workflow my_workflow { input { File fq } String basename_of_fq = basename(fq) call do_something { input: fastq = fq, ## example of referring to workflow-level input basename_of_fq = basename_of_fq } } Tasks and workflows also typically have outputs. The task-level outputs can be accessed by the workflow or any subsequent tasks. The workflow-level outputs represent the final output of the overall workflow. version 1.0 task do_something { input { File fastq String basename_of_fq } command &lt;&lt;&lt; echo &quot;First ten lines of ~{basename_of_fq}: &quot; &gt;&gt; output.txt head ~{fastq} &gt;&gt; output.txt &gt;&gt;&gt; output { File first_ten_lines = &quot;output.txt&quot; ## output variable for task } } workflow my_workflow { input { File fq } String basename_of_fq = basename(fq) call do_something { input: fastq = fq, basename_of_fq = basename_of_fq } output { File ten_lines = do_something.first_ten_lines ## referring to task output here } } 1.3 Using JSONs to control workflow inputs Running a WDL workflow generally requires two files: A .wdl file, which contains the actual workflow, and a .json file, which provides the inputs for the workflow. In the example we showed earlier, the workflow takes in a file referred to by the input variable fq. This needs to be provided by the user. Typically, this is done with a JSON file. Here’s what a JSON file for this workflow might look like: { &quot;my_workflow.fq&quot;: &quot;./data/example.fq&quot; } JSON files consist of key-value pairs. In this case, the key is \"my_workflow.fq\" and the value is the path \"./data/example.fq\". The first part of the key is the name of the workflow as written in the WDL file, in this case my_workflow. The variable being represented is referred to its name, in this case, fq. So, in the above example, the file located at the path ./data/example.fq is being input as a variable called fq into the workflow named my_workflow. Files aren’t the only type of variable you can refer to when using JSONs. Here’s an example JSON for every common WDL variable type. { &quot;some_workflow.file&quot;: &quot;./data/example.fq&quot;, &quot;some_workflow.string&quot;: &quot;Hello world!&quot;, &quot;some_workflow.integer&quot;: 1965, &quot;some_workflow.float&quot;: 3.1415, &quot;some_workflow.boolean&quot;: true, &quot;some_workflow.array_of_files&quot;: [&quot;./data/example01.fq&quot;, &quot;./data/example02.fq&quot;] } Resources: For more basic examples of WDL workflow with a single task, we recommend OpenWDL’s documentation on WDL’s base structure. For more information on types in WDL, we recommend OpenWDL’s documentation on variable types. If you are having difficulty writing valid JSON files, considering using https://jsonlint.com/ to check your JSON for any errors. 1.4 Running WDL via a computing engine In order to run a WDL workflow, we need a computing engine to execute it. The two most popular WDL executors are miniwdl and Cromwell. Both computing engines can run WDLs on multiple configurations: A local machine A High Performance Computing (HPC) Cluster A Cloud Computing backend (such as AWS/Terra/DNAnexus). If you are trying to run WDL at Fred Hutch Cancer Center’s HPC system, you should use the PROOF executor. If you are computing on a HPC or the Cloud, you should find the best practice of running a WDL computing engine based on your institution’s information technology system. If you are computing locally on your computer, below is a short guide on how to set that up. How to run simple workflows locally Not every WDL workflow will run well on a laptop, but it can be helpful to have a basic setup for testing and catching simple syntax errors. Let’s quickly set up a WDL executor to run our WDLs. In this course, we will be using miniwdl, but everything in this course will also be compatible with Cromwell unless explicitly stated otherwise. Additionally, almost all WDLs use Docker images, so you will also need to install Docker or a Docker-like alternative. Installing Docker and miniwdl is not required to use this course. We don’t want anybody to get stuck here! If you already have a method for submitting workflows, such as Terra, feel free to use that for this course instead of running workflows directly on your local machine. If you don’t have any way of running workflows at the moment, that’s also okay – we have provided plenty of examples for following along. 1.4.1 Installing Docker Note: Although Docker’s own docs recommend installing Docker Desktop for Linux, it has been reported that some WDL executors work better on Linux when installing only Docker Engine (aka Docker CE). To install Docker on your machine, follow the instructions specific to your operating system on Docker’s website. To specifically install only Docker Engine, use these instructions instead. If you are unable to install Docker on your machine, Dockstore (not affiliated with Docker) provides some experimental alternatives. Dockstore also provides a comprehensive introduction to Docker itself, including how to write a Dockerfile. Much of that information is outside the scope of this WDL-focused course, but it may be helpful for those looking to eventually create their own Docker images. 1.4.2 Installing miniwdl miniwdl is based on Python. If you do not already have Python 3.6 or higher installed, you can install Python from here. Once Python is installed on your system, you can run pip3 install miniwdl from the command line to install miniwdl. For those who prefer to use conda, use conda install -c conda-forge miniwdl instead. Once miniwdl is installed, you can verify it works properly by running miniwdl run_self_test. This will run a built-in hello world workflow. For more information, see miniwdl’s GitHub repository. 1.4.3 Launching a workflow locally with miniwdl The generic method for running a WDL with miniwdl is the following: miniwdl run [path_to_wdl_file] -i [path_to_inputs_json] If you have successfully installed miniwdl, create the following WDL file and name it greetings.wdl: version 1.0 task greet { input { String user } command &lt;&lt;&lt; echo &quot;Hello ~{user}!&quot; &gt; greets.txt &gt;&gt;&gt; output { String greeting = read_string(&quot;greets.txt&quot;) } } workflow my_workflow { input { String username } call greet { input: user = username } } Next, use this JSON file (or create one of your own) to provide the string that the workflow expects, and call the JSON file greetings.json: { &quot;my_workflow.username&quot;: &quot;Ash&quot; } On the command line, run the following: miniwdl run greetings.wdl -i greetings.json Once the task completes, you should see something like this in your command line: [timestamp] wdl.w:my_workflow finish :: job: &quot;call-greet&quot; [timestamp] wdl.w:my_workflow done { &quot;dir&quot;: &quot;[working directory]/[timestamp]_my_workflow&quot;, &quot;outputs&quot;: { &quot;my_workflow.greet.greeting&quot;: &quot;Hello Ash!&quot; } } Where [timestamp] is the date and time that you are running the workflow, and [working directory] is the working directory that you are running the workflow from. For example: 2023-12-27 13:54:12.209 wdl.w:my_workflow finish :: job: &quot;call-greet&quot; 2023-12-27 13:54:12.210 wdl.w:my_workflow done { &quot;dir&quot;: &quot;/Users/ash/github/WDL_Workflows_Guide/resources/20231227_135400_my_workflow&quot;, &quot;outputs&quot;: { &quot;my_workflow.greet.greeting&quot;: &quot;Hello Ash!&quot; } } 1.4.4 Troubleshooting 1.4.4.1 DockerException If you are seeing a verbose error message that begins with text like this: 2023-12-27 13:43:37.525 wdl.w:my_workflow.t:call-greet task greet (greetings.wdl Ln 3 Col 1) failed :: dir: &quot;/Users/sammy/github/WDL_Workflows_Guide/resources/20231227_134337_my_workflow/call-greet&quot;, error: &quot;DockerException&quot;, message: &quot;Error while fetching server API version: (&#39;Connection aborted.&#39;, FileNotFoundError(2, &#39;No such file or directory&#39;))&quot;, traceback: [&quot;Traceback (most recent call last):&quot;, &quot; File \\&quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/urllib3/connectionpool.py\\&quot;, line 790, in urlopen&quot;, &quot; response = self._make_request(&quot;, &quot; ^^^^^^^^^^^^^^^^^^^&quot;, &quot; File \\&quot;/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/urllib3/connectionpool.py\\&quot;, This is likely caused by miniwdl being unable to connect to Docker Daemon, the underlying technology that runs Docker images. This is necessary with miniwdl even though our example WDL does not specify a Docker image. Make sure you have Docker installed correctly, and make sure Docker is actively running on your machine. If you installed Docker Desktop, simply opening the Docker Desktop app should start Docker Engine. If you installed Docker without Docker Desktop, running dockerd in your command-line should start it. Be aware that starting the Docker Daemon may take a few minutes. 1.4.4.2 Missing required inputs If you forget to add -i greetings.json to your call, you will see something like this: my_workflow (greetings.wdl) --------------------------- required inputs: String username outputs: String greet.greeting missing required inputs for my_workflow: username You may also see this error if you remember to include a JSON file, but it is missing a required input. 1.4.4.3 Check JSON input If you see an error message like this: check JSON input; unknown input/output: greetings.username Double-check your input JSON. The first part of your JSON’s keys refer to the name of the workflow in the WDL file, not the filename of the WDL itself. Even though our WDL is saved as greetings.wdl, within that file, the workflow is named my_workflow. This means that the input JSON must say \"my_workflow.username\", not \"greetings.username\". Other common issues with JSON files are mistyping input variables (such as \"my_workflow.ussername\") or forgetting to enclose strings in quotation marks. When in doubt, try using https://jsonlint.com/ to check your input JSON, and double-check the name of your input variables. Loading… "],["defining-a-workflow-plan.html", "Chapter 2 Defining a workflow plan 2.1 Somatic mutation calling workflow 2.2 Workflow testing strategy 2.3 Test samples", " Chapter 2 Defining a workflow plan Our WDL guide will center around building a workflow from scratch. As we build out this workflow step-by-step, you will see what strategies and resources are used to develop a workflow that is iterative, reproducible, and efficient in terms of time and resource used. The goal is to use this workflow to illustrate common lessons in writing WDL workflows. 2.1 Somatic mutation calling workflow The workflow used as the example here is tailored to detect somatic mutations in two tumor samples. Initially, the workflow takes as input FASTQ-formatted sequencing data from two tumor specimens and one normal sample (a single normal sample is used here, but typically each tumor might have its own associated normal). Subsequently, it aligns the FASTQ files of each sample with the human reference genome (hg19), proceeds to identify and mark PCR duplicates, and conducts base quality recalibration. Following these steps, the workflow engages in somatic mutation calling, operating in a paired mode, to pinpoint mutations unique to the tumor samples in comparison to the normal one. Concluding the process, the workflow undertakes the annotation of the identified mutations, enriching the dataset with additional insights into the nature of the mutations detected. The workflow diagram: The tasks involved: Task Function Inputs Outputs BwaMem aligns the samples to the reference genome (hg19) FASTA (.fasta) file BAM (.bam) file MarkDuplicates marks PCR duplicates BAM (.bam) file BAM (.bam) file ApplyBaseRecalibrator performs base quality recalibration BAM (.bam) file BAM (.bam) file Mutect2 performs paired somatic mutation calling BAM (.bam) file VCF (.vcf) file annovar annotates the called somatic mutations VCF (.vcf) file VCF (.vcf) file 2.2 Workflow testing strategy As we build out our workflow, how can we verify that it is running correctly besides getting a message such as “Workflow finished with status ‘Succeeded’” or an exit code 0? In software development, it is essential to test your code to see whether it generates the expected output given a specified input. This principle applies into bioinformatics workflow development also: Unit Testing: We need to incorporate tests to ensure that each task we develop is correct. End-to-end testing: When we connect all the tasks together to form a workflow, we test that the workflow running end-to-end is correct. Here are some guidelines for any form of testing: The data you use for testing needs to be representative of “real” data. You should have an expectation of what the resulting output is before you run your workflow on it. It can be as specific as a MD5 checksum, or vague such as a certain file format. The process is quick to run, ideally in the range of just a few minutes. This often means using a small subset of actual data. The data you use for testing is ideally open access so others can verify your workflow also. 2.3 Test samples To serve as an example we use here whole exome sequencing data from three cell lines from the Cancer Cell Line Encyclopedia. 2.3.1 Tumor 1 : HCC4006 HCC4006 is a lung cancer cell line that has a mutation in the gene EGFR (Epithelial Growth Factor Receptor), a proto-oncogene. Mutations in EGFR result in the abnormal constitutive activation of the EGFR signaling pathway and drive cancer. In this cell-line specifically, the EGFR mutation is an in-frame deletion in Exon 19. This mutation results in the constitutive activation of the EGFR protein and is therefore oncogenic. 2.3.2 Tumor 2 : CALU1 CALU1 is a lung cancer cell line that has a mutation in the gene KRAS (Kirsten rat sarcoma viral oncogene homolog) . KRAS is also a proto-oncogene and the most common cancer-causing mutations lock the protein in an active conformation. Constitutive activation of KRAS results in carcinogenesis. In this cell-line KRAS has a point/missense mutation resulting in the substitution of the amino acid glycine (G) with cysteine (C) at position 12 of the KRAS protein (commonly known as the KRAS G12C mutation). This mutation results in the constitutive activation of KRAS and drives carcinogenesis. 2.3.3 Normal : MOLM13 MOLM 13 is a human leukemia cell line commonly used in research. While it is also a cancer cell line for the purposes of this workflow example we are going to consider it as a “normal”. This cell line does not have mutations in EGFR nor in KRAS and therefore is a practical surrogate in lieu of a conventional normal sample 2.3.4 Test data details Fastq files for all these three samples were derived from their respective whole exome sequencing. However, for the purpose of this guide we have limited the sequencing reads to span +/- 200 bp around the mutation sites for both genes. In doing so we are able to shrink the data files for quick testing. 2.3.5 Access to files All files needed to run the workflow can be downloaded here. Loading… "],["the-first-task.html", "Chapter 3 The first task 3.1 Inputs 3.2 Runtime attributes 3.3 Outputs 3.4 The whole task 3.5 Putting the workflow together 3.6 Testing your first task", " Chapter 3 The first task Before we write any sort of WDL – whether it is for somatic mutation calling like we will be going over, or any other bioinformatics task – we need to understand the building blocks of WDL: Tasks! As mentioned in the first part of this course, every WDL workflow is made up of at least one task. A task typically has inputs, outputs, runtime attributes, and a command section. You can think of a task as a discrete step in a workflow. It can involve a single call to a single bioinformatics tool, a sequence of bash commands, an inline Python script… almost anything you can do non-interactively in a terminal, you can do in a WDL task. In this section, we will go over the parts of a WDL task in more detail to help us write a task for somatic mutation calling. 3.1 Inputs The inputs of a task are the files and/or variables you will passing into your task’s command section. Typically, you will want to include at least one File input in a task, but that isn’t a requirement. You can pass most WDL variable types into a task. In our example workflow, we are starting with a single fastq file per sample, and we know we will need to convert it into a sam file. A sam file is an alignment, so we will need a reference genome to align our fastqs to. We also want to be able to control the threading for this task. Our first task’s inputs will therefore start out looking like this: task some_aligner { input { File input_fastq File ref_fasta } [...] } For some aligners, this would be a sufficient set of inputs, but we have decided to use bwa mem in particular to take us from .fastq to .sam. bwa mem requires a lot of index files, which we will also need as inputs. These index files can be specified via an array, but for now we’ll list everything separately to make sure nothing is being left out. task BwaMem { input { # main input File input_fastq # reference files File ref_fasta File ref_fasta_index File ref_dict File ref_amb File ref_ann File ref_bwt File ref_pac File ref_sa } [...] } 3.1.1 Referencing inputs in the command section The command section of a WDL task is a bash script that will be run non-interactively by the WDL executor. Although it is helpful to think of tasks as discrete steps in a workflow, that does not mean each task needs to be a single line of code or use only one piece of software. You could, for example, call a bioinformatics tool and then reprocess the outputs in the same WDL task. A WDL task’s input variables are generally referred to in the command section using a tilde (~) and curly braces, using heredoc syntax. Why use heredox syntax? You may see WDLs that use this notation for the command section in a task: task do_something_curly_braces { input { String some_string } command { ## note the brackets some_other_string=&quot;FOO&quot; echo ${some_string} echo $some_other_string } } We recommend using heredoc-style syntax instead: task do_something_carrots { input { String some_string } command &lt;&lt;&lt; ## note the &#39;&lt;&lt;&lt;&#39; some_other_string=&quot;FOO&quot; echo ~{some_string} echo $some_other_string &gt;&gt;&gt; ## closing &#39;&gt;&gt;&gt;&#39; } Heredoc-style syntax for command sections can be clearer than the alternative, as it makes a clearer distinction between bash variables and WDL variables. This is especially helpful for complicated bash scripts. Heredoc-style syntax is also what the WDL 1.1 spec recommends using in most cases. However, the older non-heredoc style is still perfectly functional for a lot of use cases. To prevent issues with spaces in String and File types, it is often a good idea to put quotation marks around a String or File variables, like so: task cowsay { input { String some_string } command &lt;&lt;&lt; cowsay -t &quot;~{some_string}&quot; &gt;&gt;&gt; } What can happen if we don’t use quotation marks around String or File variables? If some_string is “hello world” then the command section of this task is interpreted as the following: cowsay -t &quot;hello world&quot; What happens if we had not wrapped ~{some_string} in quotation marks? If some_string was just “hello”, it wouldn’t matter. But because some_string is two words with a space in between, then the script would be interpreted as cowsay -t hello world and cause an error, because the cowsay program thinks world is another argument. By including quotation marks, cowsay -t \"~{some_string}\" can be interpreted as cowsay -t \"hello world\" and you will correctly get a cow’s greeting instead of an error. Let’s see how we can reference our inputs in the command section of our task. task BwaMem { input { File input_fastq File ref_fasta # these variables may look as though they are unused... but bwa mem needs them! File ref_fasta_index File ref_dict File ref_amb File ref_ann File ref_bwt File ref_pac File ref_sa } command &lt;&lt;&lt; # warning: this will not run on all backends! see below for an explanation! bwa mem \\ -p -v 3 -t 16 -M -R &#39;@RG\\tID:foo\\tSM:foo2&#39; \\ &quot;~{ref_fasta}&quot; &quot;~{input_fastq}&quot; &gt; my_nifty_output.sam &gt;&gt;&gt; } If we were to run this task in a workflow as-is, we might expect it to run on any backend that can handle the hardware requirements. Those hardware requirements are a bit steep – the -t 16 part specifically requests 16 threads, for example – but besides that, it may look like a perfectly functional task. We have written this task to use 16 threads, as noted by -t 16 in the bwa mem call. If you are running on a backend that cannot provide 16 threads, you may need to adjust set the number of threads to a lower number. Later on in this course, we will discuss how to account for lots of different backends using common workarounds and optional variables. Unfortunately, even on backends that can provide the necessary computing power, it is quite likely this task will not run as expected. This is because of how inputs work in WDL – or, more specifically, how input files get localized when working with WDL. 3.1.2 File localization When running a WDL, a WDL executor will typically place duplicates of the input files in a brand-new subfolder of the task’s working directory. Typically, you don’t know the name of the directory before runtime – they vary depending on the backend you are running and the WDL executor itself. Thankfully, at runtime, File-type variables such as ~{input_fastq} and ~{ref_fasta} will be replaced with paths to their respective files. For example, if you were to run this workflow on a laptop using miniwdl, ~{ref_fasta} would likely end up turning into ./_miniwdl_inputs/0/ref.fa at runtime. On the other hand, if you were running the exact same workflow with Cromwell, ~{ref_fasta} would turn into something like /cromwell-executions/BwaMem/97c9341e-9322-9a2f-4f54-4114747b8fff/call-test_localization/inputs/-2022115965/ref.fa. Keep in mind that these are the paths of copies of the input files, and that sometimes input files can be in different subfolders. For example, it’s possible ~{input_fastq} would be ./_miniwdl_inputs/0/sample.fastq while ~{ref_fasta} may be ./_miniwdl_inputs/1/ref.fa. For many programs, an input file being at ./ref.fa versus /_miniwdl_inputs/0/ref.fa is inconsequential. However, this aspect of WDL can occasionally cause issues. bwa mem is a great example of the type of command where this sort of thing can go haywire without proper planning, due to the program making an assumption about some of your input files. Specifically, bwa mem assumes that the reference fasta that you pass in shares the same folder as the other reference files (ref_amb, ref_ann, ref_bwt, etc), and it does not allow you to specify otherwise. Another example of file localization issue bwa is not the only program that makes assumptions about where files are located, and assumptions being made do not only affect reference genome files. Bioinformatics programs that take in some sort of index file frequently assume that index file is located in the same directory as the non-index input. For example, if you were to pass in SAMN1234.bam into covstats, it would expect an index file named SAMN1234.bam.bai or SAMN1234.bai in the same directory as the bam file, as seen in the source code here. As there is no way to specify that the index file manually, you need to take that into consideration when writing WDLs involving covstats, bwa, and other similar tools. Thankfully, the solution here is simple: Move all of the input files directly into the working directory. task BwaMem { input { File input_fastq File ref_fasta File ref_fasta_index File ref_dict File ref_amb File ref_ann File ref_bwt File ref_pac File ref_sa } command &lt;&lt;&lt; set -eo pipefail # This can also be done by creating an array and then looping that array, # but we&#39;ll do it one line at a time or clarity&#39;s sake. mv &quot;~{ref_fasta}&quot; . mv &quot;~{ref_fasta_index}&quot; . mv &quot;~{ref_dict}&quot; . mv &quot;~{ref_amb}&quot; . mv &quot;~{ref_ann}&quot; . mv &quot;~{ref_bwt}&quot; . mv &quot;~{ref_pac}&quot; . mv &quot;~{ref_sa}&quot; . bwa mem \\ [...] &gt;&gt;&gt; } Some backends/executors do not support mv acting on input files. If you are running into problems with this and are working with miniwdl, the --copy-input-files flag will usually allow mv to work. You could also simply use cp to copy the files instead of move them, although this may not be an efficient use of disk space, so consider using mv if your target backends and executors can handle it. With our files now all in the working directory, we can turn our attention to the bwa task itself. We can no longer directly pass in ~{ref_fasta} or any of the other files we moved into the working directory, because those variables will point to a non-existent file in a now-empty input directory. There are several ways to solve this problem: Assuming the filename of an input is constant, which might be a safe assumption for reference files Using the bash built-in basename function Using the WDL built-in basename() function along with private variables We recommend using the last option, as it works for essentially any input and may be more intuitive than the bash basename function. OpenWDL explains how basename() works. The next section will provide an example of using it alongside private variables. 3.1.3 Private variables Is there a variable you wish to use in your task section that is based on another input variable, or do not want people using your workflow to be able to directly overwrite? You can define variables outside the input {} section to create variables that function like private variables. In our case, we create String ref_fasta_local as ref_fasta’s file base name to refer to the files we have moved to the working directory. We also create String base_file_name as input_fastq’s file base name and use it to name our output files, such as \"~{base_file_name}.sorted_query_aligned.bam\". The variables read_group_id, sample_name, and platform_info are created similarly. task BwaMem { input { File input_fastq File ref_fasta File ref_fasta_index File ref_dict File ref_amb File ref_ann File ref_bwt File ref_pac File ref_sa } # basename() is a built-in WDL function that acts like bash&#39;s basename String base_file_name = basename(input_fastq, &quot;.fastq&quot;) String ref_fasta_local = basename(ref_fasta) String read_group_id = &quot;ID:&quot; + base_file_name String sample_name = &quot;SM:&quot; + base_file_name String platform_info = &quot;PL:illumina&quot; command &lt;&lt;&lt; set -eo pipefail mv &quot;~{ref_fasta}&quot; . mv &quot;~{ref_fasta_index}&quot; . mv &quot;~{ref_dict}&quot; . mv &quot;~{ref_amb}&quot; . mv &quot;~{ref_ann}&quot; . mv &quot;~{ref_bwt}&quot; . mv &quot;~{ref_pac}&quot; . mv &quot;~{ref_sa}&quot; . bwa mem \\ -p -v 3 -t 16 -M -R &#39;@RG\\t~{read_group_id}\\t~{sample_name}\\t~{platform_info}&#39; \\ ~{ref_fasta_local} ~{input_fastq} &gt; ~{base_file_name}.sam samtools view -1bS -@ 15 -o ~{base_file_name}.aligned.bam ~{base_file_name}.sam samtools sort -@ 15 -o ~{base_file_name}.sorted_query_aligned.bam ~{base_file_name}.aligned.bam &gt;&gt;&gt; } 3.2 Runtime attributes The runtime attributes of a task tell the WDL executor important information about how to run the task. For a bwa mem task, we want to make sure we have plenty of hardware resources available. We also need to include a reference to the docker image we want the task to actually run in. runtime { memory: &quot;48 GB&quot; cpu: 16 docker: &quot;ghcr.io/getwilds/bwa:0.7.17&quot; disks: &quot;local-disk 100 SSD&quot; } In WDL 1.0, the interpretation of runtime attributes by different executors and backends is extremely varied. The WDL 1.0 spec allows for arbitrary values here: Individual backends will define which keys they will inspect so a key/value pair may or may not actually be honored depending on how the task is run. Values can be any expression and it is up to the engine to reject keys and/or values that do not make sense in that context. This can lead to some pitfalls: Some of the attributes in your task’s runtime section may be silently ignored, such as the memory attribute when running Cromwell on the Fred Hutch HPC (as of Feb 2024) Some runtime attributes that are unique to particular backends, such as the Fred Hutch HPC’s walltime attribute The same runtime attribute working differently on different backends, such as disks acting differently on Cromwell depending on whether it is running on AWS or GCP When writing WDL 1.0 workflows with specific hardware requirements, keep in mind what your backend and executor is able to interpret. It is also helpful to consider that other people running your workflow may be doing so on different backends and executors. More information can be found in the appendix, where we talk about designing WDLs for specific backends. For now, we will stick with memory, cpu, docker, and disks as this group of four runtime attributes will help us run this workflow on the majority of backends and executors. Even though the Fred Hutch HPC will ignore the memory and disks attributes, for instance, their inclusion will not cause the workflow to fail, but they will allow the workflow to run on Terra. Some differences between WDL 1.0 and 1.1 on Runtime attributes Although the focus of this course is on WDL 1.0, it is worth noting that in the WDL 1.1 spec, a very different approach to runtime attributes is taken: There are a set of reserved attributes (described below) that must be supported by the execution engine, and which have well-defined meanings and default values. Default values for all optional standard attributes are directly defined by the WDL specification in order to encourage portability of workflows and tasks; execution engines should NOT provide additional mechanisms to set default values for when no runtime attributes are defined. If you are writing WDLs under the WDL 1.1 standard, you may have more flexibility with runtime attributes. Be aware that as of February 2024, Cromwell does not support WDL 1.1. 3.2.1 Docker images and containers WDL is built to make use of Docker as it makes handling software dependencies much simpler. Docker images can help address all of these situations: Some software is difficult to install or compile on certain systems Some programs have conflicting dependencies You may not want to directly install software on your system to prevent it from breaking existing software You may not have permission to install software if you are using an institute HPC or other shared resource When you run a WDL task that has a docker runtime attribute, your task will be executed in a Docker container sandbox environment. This container sandbox is derived from a template called a Docker image, which packages installed software in a special file system. This is one of the main features of a Docker image – because a Docker image packages the software you need, you can skip much of the installation and dependency issues associated with using new software. Because you take actions within a Docker container sandbox, it’s unlikely for you to “mess up” your main system’s files. Although a Docker container is, strictly speaking, not the same as a virtual machine, it is helpful to think of it as one if you are new to Docker. Docker containers are managed by Docker Engine/Apptainer, and the official Docker GUI is called Docker Desktop. More information on finding and developing Docker images Although you will generally need to be able to run Docker in order to run WDLs, you do not need to know how to create Dockerfiles – plaintext files which compile Docker images when run via docker build – to write your own WDLs. Most popular bioinformatics software packages already have ready-to-use Docker images available, which you can typically find on Docker Hub. Other registries include quay.io and the Google Container Registry. With that being said, if you would like to create your own Docker images, there are many tutorials and guidelines available online. You can also learn more about the details of Docker (and why they technically aren’t virtual machines) in Docker’s official curriculum. 3.3 Outputs The outputs of a task are defined in the output section of your task. Typically, this will take the form of directly outputting a file that was created in the command section. When these file outputs are referenced in the output section, you can refer to their path in the Docker container directly. You can also make outputs a function of input variables, including private input variables. This can be helpful if you intend on running this WDL on many different files – each one will get a unique filename based on the input fastq, instead of every sample ending up being named something generic like “converted.sam”. For our bwa mem task, one way to write the output section would be as follows: output { File analysisReadySorted = &quot;~{base_file_name}.sorted_query_aligned.bam&quot; } Another way of writing this is with string concatenation. This is equivalent to what we wrote above – choose whichever version you prefer. output { File analysisReadySorted = base_file_name + &quot;.sorted_query_aligned.bam&quot; } If the output was not in the working directory, we would need to change the output to point to the file’s path relative to the working directory, such as File analysisReadySorted = \"some_folder/~{base_file_name}.sorted_query_aligned.bam\". Below are some some additional ways you can handle task outputs. Ouputs as functions of other outputs in the same task Outputs can (generally, see warning below) also be functions of other outputs in the same task, as long as those outputs are declared first. task add_one { input { Int some_integer } command &lt;&lt;&lt; echo ~{some_integer} &gt; a.txt echo &quot;1&quot; &gt; b.txt &gt;&gt;&gt; output { Int a = read_int(&quot;a.txt&quot;) Int b = read_int(&quot;b.txt&quot;) Int c = a + b } } Cromwell does not fully support outputs being a function of the same task’s other outputs. On the Terra backend, the above code example would cause an error. Grabbing multiple outputs at the same time To grab multiple outputs at the same time, use glob() to create an array of files. We’ll also take this opportunity to demonstrate iterating through a bash array created from an Array[String] input – for more information on this data type, see chapter six of this course. task one_word_per_file { input { Array[String] a_big_sentence } command &lt;&lt;&lt; ARRAY_OF_WORDS=(~{sep=&quot; &quot; a_big_sentence}) i=0 for word in &quot;${!ARRAY_OF_WORDS[@]}&quot; do i=$((i+1)) echo $word &gt;&gt; $i.txt done &gt;&gt;&gt; output { Array[File] several_words = glob(&quot;*.txt&quot;) } } glob() can also be used to grab just one file via glob(\"*.txt\")[0] to grab the first thing that matches the glob. This is usually only necessary if you know the extension of an output, but do not have a way of predicting the rest of its filename. Be aware that if anything else in the working directory has the extension you are searching for, you might accidentally grab that one instead of the one you are looking for! 3.4 The whole task We’ve now designed a bwa mem task that can run on almost any backend that supports WDL and can handle the hardware requirements. Issues involving bwa mem expecting reference files to be in the same folder and/or putting output files into input folders have been sidestepped thanks to careful design and consideration. The runtime section clearly defines the expected hardware requirements, and the outputs section defines what we expect the task to give us when all is said and done. We’re now ready to continue with the rest of our workflow. task BwaMem { input { File input_fastq referenceGenome refGenome } String base_file_name = basename(input_fastq, &quot;.fastq&quot;) String ref_fasta_local = basename(refGenome.ref_fasta) String read_group_id = &quot;ID:&quot; + base_file_name String sample_name = &quot;SM:&quot; + base_file_name String platform_info = &quot;PL:illumina&quot; command &lt;&lt;&lt; set -eo pipefail mv ~{refGenome.ref_fasta} . mv ~{refGenome.ref_fasta_index} . mv ~{refGenome.ref_dict} . mv ~{refGenome.ref_amb} . mv ~{refGenome.ref_ann} . mv ~{refGenome.ref_bwt} . mv ~{refGenome.ref_pac} . mv ~{refGenome.ref_sa} . bwa mem \\ -p -v 3 -t 16 -M -R &#39;@RG\\t~{read_group_id}\\t~{sample_name}\\t~{platform_info}&#39; \\ ~{ref_fasta_local} ~{input_fastq} &gt; ~{base_file_name}.sam samtools view -1bS -@ 15 -o ~{base_file_name}.aligned.bam ~{base_file_name}.sam samtools sort -@ 15 -o ~{base_file_name}.sorted_query_aligned.bam ~{base_file_name}.aligned.bam &gt;&gt;&gt; output { File analysisReadySorted = &quot;~{base_file_name}.sorted_query_aligned.bam&quot; } runtime { memory: &quot;48 GB&quot; cpu: 16 docker: &quot;ghcr.io/getwilds/bwa:0.7.17&quot; } } 3.5 Putting the workflow together A workflow is needed to run the BwaMem task we just built. The workflow’s input variables are defined by the workflow JSON metadata, and are then passed on as inputs in our BwaMem call. When the BwaMem call is complete, the workflow’s output File variable is defined based on the task’s output. Lastly, we have a parameter_meta component in our workflow that describes each workflow input variable as documentation. For the workflow to actually “see” the task, the task will either need to be imported at the top of the workflow (just under the version 1.0 string), or included in the same file as the workflow. For simplicity, we will put the workflow and the task in the same file. 3.6 Testing your first task To test your first task and your workflow, you should have expectation of output is. For this first BwaMem task, we just care that the BAM file is created with aligned reads. You can use samtools view output.sorted_query_aligned.bam to examine the reads and pipe it to wordcount wc to get the number of total reads. This number should be almost identical as the number of reads from your input FASTQ file if you run wc input.fastq. In other tasks, we might have a more precise expectation of what the output file should be, such as containing the specific somatic mutation call that we have curated. Here is an example JSON with the test data needed to run this single-task workflow: { &quot;mutation_calling.sampleFastq&quot;: &quot;/path/to/Tumor_2_EGFR_HCC4006_combined.fastq&quot;, &quot;mutation_calling.ref_fasta&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta&quot;, &quot;mutation_calling.ref_fasta_index&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.fai&quot;, &quot;mutation_calling.ref_dict&quot;: &quot;/path/to/Homo_sapiens_assembly19.dict&quot;, &quot;mutation_calling.ref_pac&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.pac&quot;, &quot;mutation_calling.ref_sa&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.sa&quot;, &quot;mutation_calling.ref_amb&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.amb&quot;, &quot;mutation_calling.ref_ann&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.ann&quot;, &quot;mutation_calling.ref_bwt&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.bwt&quot; } The example JSON using the Fred Hutch HPC { “mutation_calling.sampleFastq”: “/fh/fast/paguirigan_a/pub/ReferenceDataSets/workflow_testing_data/WDL/wdl_101/HCC4006_final.fastq”, “mutation_calling.ref_fasta”: “/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta”, “mutation_calling.ref_fasta_index”: “/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.fai”, “mutation_calling.ref_dict”: “/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.dict”, “mutation_calling.ref_pac”: “/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.pac”, “mutation_calling.ref_sa”: “/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.sa”, “mutation_calling.ref_amb”: “/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.amb”, “mutation_calling.ref_ann”: “/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.ann”, “mutation_calling.ref_bwt”: “/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.bwt” } Loading… "],["connecting-multiple-tasks-together-in-a-linear-chain.html", "Chapter 4 Connecting multiple tasks together in a linear chain 4.1 How to connect tasks together in a workflow 4.2 Writing MarkDuplicates task 4.3 The rest of the linear chain workflow", " Chapter 4 Connecting multiple tasks together in a linear chain Now that you have a first task in a workflow up and running, the next step is to continue building out the workflow as described in our Workflow Plan: BwaMem aligns the samples to the reference genome. MarkDuplicates marks PCR duplicates. ApplyBaseRecalibrator applies base recalibration. Mutect2 performs somatic mutation calling. For this current iteration, we start with Mutect2TumorOnly which only uses the tumor sample. Later on, we will switch to a version that will allow for comparing tumor samples against a non-tumor normal sample. annovar annotates the called somatic mutations. We do this via a linear chain, in which we feed the output of one task to the input of the next task. Let’s see how to build a linear chain in a workflow. 4.1 How to connect tasks together in a workflow We can easily connect tasks together because of the following: The output variables of a task can be accessed at the workflow level as inputs for the subsequent task. For instance, let’s see the output of our BwaMem task. output { File analysisReadyBam = &quot;~{base_file_name}.aligned.bam&quot; File analysisReadySorted = &quot;~{base_file_name}.sorted_query_aligned.bam&quot; } The File variables analysisReadyBam and analysisReadySorted can now be accessed anywhere in the workflow block after the BwaMem task as BwaMem.analysisReadyBam and BwaMem.analysisReadySorted, respectively. Therefore, when we call the MarkDuplicates task, we can pass it the input BwaMem.analysisReadySorted from the BwaMem task: workflow mutation_calling { input { File sampleFastq # Reference genome File ref_fasta File ref_fasta_index File ref_dict File ref_amb File ref_ann File ref_bwt File ref_pac File ref_sa } # Map reads to reference call BwaMem { input: input_fastq = sampleFastq, ref_fasta = ref_fasta, ref_fasta_index = ref_fasta_index, ref_dict = ref_dict, ref_amb = ref_amb, ref_ann = ref_ann, ref_bwt = ref_bwt, ref_pac = ref_pac, ref_sa = ref_sa } call MarkDuplicates { input: input_bam = BwaMem.analysisReadySorted } } Resources: For a basic introduction to linear chain, see OpenWDL Docs’ introduction. To see other examples of linear chain and variations, see OpenWDL Docs’s section on workflow plumbing. 4.2 Writing MarkDuplicates task Of course, the task MarkDuplicates hasn’t been written yet! Let’s go through it together: 4.2.1 Input The task takes an input bam file that has been aligned to the reference genome. It needs to be a File input_bam based on how we introduced it in the workflow above. That is easy to write up: task MarkDuplicates { input { File input_bam } } 4.2.2 Private variables in the task Similar to the BwaMem task, we will name our output files based on the base name of the original input file in the workflow. Therefore, it makes sense to create a private String variable base_file_name that contains this base name of input_bam. We will use base_file_name in the Command section to specify the output bam and metrics files. task MarkDuplicates { input { File input_bam } String base_file_name = basename(input_bam, &quot;.sorted_query_aligned.bam&quot;) } 4.2.3 Command As we have been doing all along, we refer to any defined variables from the input or private variables using ~{this} syntax. task MarkDuplicates { input { File input_bam } String base_file_name = basename(input_bam, &quot;.sorted_query_aligned.bam&quot;) command &lt;&lt;&lt; gatk MarkDuplicates \\ --INPUT &quot;~{input_bam}&quot; \\ --OUTPUT &quot;~{base_file_name}.duplicates_marked.bam&quot; \\ --METRICS_FILE &quot;~{base_file_name}.duplicate_metrics&quot; \\ --CREATE_INDEX true \\ --OPTICAL_DUPLICATE_PIXEL_DISTANCE 100 \\ --VALIDATION_STRINGENCY SILENT &gt;&gt;&gt; } 4.2.4 Runtime and Output We specify a different Docker image that contains the GATK software, and the relevant computing needs. We also specify three different output files, two of which are specified in the command section, and the third is a bam index file that is automatically created by the command section. Below is the task all together. It has a form very similar to our first task BwaMem. task MarkDuplicates { input { File input_bam } String base_file_name = basename(input_bam, &quot;.sorted_query_aligned.bam&quot;) command &lt;&lt;&lt; gatk MarkDuplicates \\ --INPUT &quot;~{input_bam}&quot; \\ --OUTPUT &quot;~{base_file_name}.duplicates_marked.bam&quot; \\ --METRICS_FILE &quot;~{base_file_name}.duplicate_metrics&quot; \\ --CREATE_INDEX true \\ --OPTICAL_DUPLICATE_PIXEL_DISTANCE 100 \\ --VALIDATION_STRINGENCY SILENT &gt;&gt;&gt; runtime { docker: &quot;broadinstitute/gatk:4.1.4.0&quot; memory: &quot;48 GB&quot; cpu: 4 } output { File markDuplicates_bam = &quot;~{base_file_name}.duplicates_marked.bam&quot; File markDuplicates_bai = &quot;~{base_file_name}.duplicates_marked.bai&quot; File duplicate_metrics = &quot;~{base_file_name}.duplicate_metrics&quot; } } 4.2.5 Testing the workflow As before, when you add a new task to the workflow, you should always test that it works on your test sample. To check that MarkDuplicates is indeed marking PCR duplicates, you could check for the presence of the PCR duplicate flag in reads, which has a decimal value of 1024 in the SAM Flags Field. 4.3 The rest of the linear chain workflow We build out the rest of the tasks in a very similar fashion. Tasks ApplyBaseRecalibrator and Mutect2TumorOnly both have files that need to be localized, but otherwise all the tasks have a very similar form as BwaMem and MarkDuplicates. For this current iteration, we use only the tumor sample for mutation calling. In the following chapters, we will use additional WDL features to make use of tumor and normal samples for mutation calling. We also expand our input JSON metadata to have files needed for each task: { &quot;mutation_calling.sampleFastq&quot;: &quot;/path/to/Tumor_2_EGFR_HCC4006_combined.fastq&quot;, &quot;mutation_calling.ref_fasta&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta&quot;, &quot;mutation_calling.ref_fasta_index&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.fai&quot;, &quot;mutation_calling.ref_dict&quot;: &quot;/path/to/Homo_sapiens_assembly19.dict&quot;, &quot;mutation_calling.ref_pac&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.pac&quot;, &quot;mutation_calling.ref_sa&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.sa&quot;, &quot;mutation_calling.ref_amb&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.amb&quot;, &quot;mutation_calling.ref_ann&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.ann&quot;, &quot;mutation_calling.ref_bwt&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.bwt&quot;, &quot;mutation_calling.ref_name&quot;: &quot;hg19&quot;, &quot;mutation_calling.dbSNP_vcf_index&quot;: &quot;/path/to/dbsnp_138.b37.vcf.gz.tbi&quot;, &quot;mutation_calling.dbSNP_vcf&quot;: &quot;/path/to/dbsnp_138.b37.vcf.gz&quot;, &quot;mutation_calling.known_indels_sites_indices&quot;: &quot;/path/to/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.idx&quot;, &quot;mutation_calling.known_indels_sites_VCFs&quot;: &quot;/path/to/Mills_and_1000G_gold_standard.indels.b37.sites.vcf&quot;, &quot;mutation_calling.af_only_gnomad&quot;: &quot;/path/to/af-only-gnomad.raw.sites.b37.vcf.gz&quot;, &quot;mutation_calling.af_only_gnomad_index&quot;: &quot;/path/to/af-only-gnomad.raw.sites.b37.vcf.gz.tbi&quot;, &quot;mutation_calling.annovar_protocols&quot;: &quot;refGene,knownGene,cosmic70,esp6500siv2_all,clinvar_20180603,gnomad211_exome&quot;, &quot;mutation_calling.annovar_operation&quot;: &quot;g,f,f,f,f,f&quot; } The JSON using the Fred Hutch HPC { &quot;mutation_calling.sampleFastq&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/workflow_testing_data/WDL/wdl_101/HCC4006_final.fastq&quot;, &quot;mutation_calling.ref_fasta&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta&quot;, &quot;mutation_calling.ref_fasta_index&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.fai&quot;, &quot;mutation_calling.ref_dict&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.dict&quot;, &quot;mutation_calling.ref_pac&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.pac&quot;, &quot;mutation_calling.ref_sa&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.sa&quot;, &quot;mutation_calling.ref_amb&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.amb&quot;, &quot;mutation_calling.ref_ann&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.ann&quot;, &quot;mutation_calling.ref_bwt&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.bwt&quot;, &quot;mutation_calling.ref_name&quot;: &quot;hg19&quot;, &quot;mutation_calling.dbSNP_vcf_index&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/dbsnp_138.b37.vcf.gz.tbi&quot;, &quot;mutation_calling.dbSNP_vcf&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/dbsnp_138.b37.vcf.gz&quot;, &quot;mutation_calling.known_indels_sites_indices&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.idx&quot;, &quot;mutation_calling.known_indels_sites_VCFs&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Mills_and_1000G_gold_standard.indels.b37.sites.vcf&quot;, &quot;mutation_calling.af_only_gnomad&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/af-only-gnomad.raw.sites.b37.vcf.gz&quot;, &quot;mutation_calling.af_only_gnomad_index&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/af-only-gnomad.raw.sites.b37.vcf.gz.tbi&quot;, &quot;mutation_calling.annovar_protocols&quot;: &quot;refGene,knownGene,cosmic70,esp6500siv2_all,clinvar_20180603,gnomad211_exome&quot;, &quot;mutation_calling.annovar_operation&quot;: &quot;g,f,f,f,f,f&quot; } Loading… "],["organizing-variables-via-structs.html", "Chapter 5 Organizing variables via Structs", " Chapter 5 Organizing variables via Structs In our workflow so far, we see that certain variables are always used together, even for different tasks. For example, variables related to the reference genome are always used for the same purpose and passed on to tasks in almost the same way. This leads to quite a bit of coding redundancy, as when we write down the large set of variables related to the reference genome as task inputs, we are just thinking about one entity. We don’t make distinctions of the reference genome files until the task body itself. To improve code organization and readability, we can package all variables related to the reference genome into a compound data structure called a struct. With a struct variable, we can refer all the packaged variables as one single variable, and also refer to specific variables within the struct without losing any information. OpenWDL Docs also has an excellent introduction and examples on structs. To define a struct, we must declare it outside of a workflow and task: struct referenceGenome { File ref_fasta File ref_fasta_index File ref_dict File ref_amb File ref_ann File ref_bwt File ref_pac File ref_sa String ref_name } workflow mutation_calling { input { File sampleFastq referenceGenome refGenome ## our struct ... } # Map reads to reference call BwaMem { input: input_fastq = sampleFastq, refGenome = refGenome ## our struct } } The referenceGenome struct contains all the variables related to the reference genome, but values cannot be defined here. The struct definition merely lays the skeleton components of the data structure, but contains no actual values. In our workflow inputs, we remove all of the File variables associated with reference genome definitions, but keep anything that isn’t related to the reference genome, such as sampleFastq. We instead declare a referenceGenome struct variable called refGenome via referenceGenome refGenome. We can access the variables within a struct by the following syntax: structVar.varName, such as refGenome.ref_name. The WDL spec has more information on how to define and use structs. To give values to refGenome, we need to modify our JSON metadata file. We define the refGenome variable in a nested structure that corresponds to the referenceGenome struct. Let’s take a look: { &quot;mutation_calling.refGenome&quot;: { &quot;ref_fasta&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta&quot;, &quot;ref_fasta_index&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.fai&quot;, &quot;ref_dict&quot;: &quot;/path/to/Homo_sapiens_assembly19.dict&quot;, &quot;ref_pac&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.pac&quot;, &quot;ref_sa&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.sa&quot;, &quot;ref_amb&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.amb&quot;, &quot;ref_ann&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.ann&quot;, &quot;ref_bwt&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.bwt&quot;, &quot;ref_name&quot;: &quot;hg19&quot; }, &quot;mutation_calling.dbSNP_vcf_index&quot;: &quot;/path/to/dbsnp_138.b37.vcf.gz.tbi&quot;, ... } Now refGenome has all the values it needs for our tasks. In addition, we have replaced all the reference genome inputs in call BwaMem with refGenome in order to pass information to a task via structs. Within the BwaMem task, we must refer to variables inside the struct, such as refGenome.ref_name (which has a value of “hg19” using this JSON metadata): # Align fastq file to the reference genome task BwaMem { input { File input_fastq referenceGenome refGenome } String base_file_name = basename(input_fastq, &quot;.fastq&quot;) String ref_fasta_local = basename(refGenome.ref_fasta) String read_group_id = &quot;ID:&quot; + base_file_name String sample_name = &quot;SM:&quot; + base_file_name String platform_info = &quot;PL:illumina&quot; command &lt;&lt;&lt; set -eo pipefail mv &quot;~{refGenome.ref_fasta}&quot; . mv &quot;~{refGenome.ref_fasta_index}&quot; . mv &quot;~{refGenome.ref_dict}&quot; . mv &quot;~{refGenome.ref_amb}&quot; . mv &quot;~{refGenome.ref_ann}&quot; . mv &quot;~{refGenome.ref_bwt}&quot; . mv &quot;~{refGenome.ref_pac}&quot; . mv &quot;~{refGenome.ref_sa}&quot; . bwa mem \\ -p -v 3 -t 16 -M -R &#39;@RG\\t~{read_group_id}\\t~{sample_name}\\t~{platform_info}&#39; \\ &quot;~{ref_fasta_local}&quot; &quot;~{input_fastq}&quot; &gt; &quot;~{base_file_name}.sam&quot; samtools view -1bS -@ 15 -o &quot;~{base_file_name}.aligned.bam&quot; &quot;~{base_file_name}.sam&quot; samtools sort -@ 15 -o &quot;~{base_file_name}.sorted_query_aligned.bam&quot; &quot;~{base_file_name}.aligned.bam&quot; &gt;&gt;&gt; output { File analysisReadySorted = &quot;~{base_file_name}.sorted_query_aligned.bam&quot; } runtime { memory: &quot;48 GB&quot; cpu: 16 docker: &quot;ghcr.io/getwilds/bwa:0.7.17&quot; } } Other tasks in the workflow, such as ApplyBaseRecalibrator and Mutect2TumorOnly also make use of the reference genome, so we pass refGenome to it. The final task annovar only requires the reference genome name, and none of the files in the referenceGenome struct. We make a stylistic choice to pass only refGenome.ref_name to the input of annovar task call, as the task doesn’t need the full information of the struct. This stylistic choice is based on the principle of passing on the minimally needed information for a modular piece of code to run, which makes the task annovar depend on the minimal amount of inputs. This will also save us time and disk space by not having to localize several gigabytes of reference files into the Docker container that annovar will be running in. call annovar { input: input_vcf = Mutect2TumorOnly.output_vcf, ref_name = refGenome.ref_name, annovar_operation = annovar_operation, annovar_protocols = annovar_protocols } Putting everything together in the workflow: The JSON metadata: { &quot;mutation_calling.sampleFastq&quot;: &quot;/path/to/Tumor_2_EGFR_HCC4006_combined.fastq&quot;, &quot;mutation_calling.refGenome&quot;: { &quot;ref_fasta&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta&quot;, &quot;ref_fasta_index&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.fai&quot;, &quot;ref_dict&quot;: &quot;/path/to/Homo_sapiens_assembly19.dict&quot;, &quot;ref_pac&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.pac&quot;, &quot;ref_sa&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.sa&quot;, &quot;ref_amb&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.amb&quot;, &quot;ref_ann&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.ann&quot;, &quot;ref_bwt&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.bwt&quot;, &quot;ref_name&quot;: &quot;hg19&quot; }, &quot;mutation_calling.dbSNP_vcf_index&quot;: &quot;/path/to/dbsnp_138.b37.vcf.gz.tbi&quot;, &quot;mutation_calling.dbSNP_vcf&quot;: &quot;/path/to/dbsnp_138.b37.vcf.gz&quot;, &quot;mutation_calling.known_indels_sites_indices&quot;: &quot;/path/to/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.idx&quot;, &quot;mutation_calling.known_indels_sites_VCFs&quot;: &quot;/path/to/Mills_and_1000G_gold_standard.indels.b37.sites.vcf&quot;, &quot;mutation_calling.af_only_gnomad&quot;: &quot;/path/to/af-only-gnomad.raw.sites.b37.vcf.gz&quot;, &quot;mutation_calling.af_only_gnomad_index&quot;: &quot;/path/to/af-only-gnomad.raw.sites.b37.vcf.gz.tbi&quot;, &quot;mutation_calling.annovar_protocols&quot;: &quot;refGene,knownGene,cosmic70,esp6500siv2_all,clinvar_20180603,gnomad211_exome&quot;, &quot;mutation_calling.annovar_operation&quot;: &quot;g,f,f,f,f,f&quot; } The JSON using the Fred Hutch HPC { &quot;mutation_calling.sampleFastq&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/workflow_testing_data/WDL/wdl_101/HCC4006_final.fastq&quot;, &quot;mutation_calling.refGenome&quot;: { &quot;ref_fasta&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta&quot;, &quot;ref_fasta_index&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.fai&quot;, &quot;ref_dict&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.dict&quot;, &quot;ref_pac&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.pac&quot;, &quot;ref_sa&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.sa&quot;, &quot;ref_amb&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.amb&quot;, &quot;ref_ann&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.ann&quot;, &quot;ref_bwt&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.bwt&quot;, &quot;ref_name&quot;: &quot;hg19&quot; }, &quot;mutation_calling.dbSNP_vcf_index&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/dbsnp_138.b37.vcf.gz.tbi&quot;, &quot;mutation_calling.dbSNP_vcf&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/dbsnp_138.b37.vcf.gz&quot;, &quot;mutation_calling.known_indels_sites_indices&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.idx&quot;, &quot;mutation_calling.known_indels_sites_VCFs&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Mills_and_1000G_gold_standard.indels.b37.sites.vcf&quot;, &quot;mutation_calling.af_only_gnomad&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/af-only-gnomad.raw.sites.b37.vcf.gz&quot;, &quot;mutation_calling.af_only_gnomad_index&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/af-only-gnomad.raw.sites.b37.vcf.gz.tbi&quot;, &quot;mutation_calling.annovar_protocols&quot;: &quot;refGene,knownGene,cosmic70,esp6500siv2_all,clinvar_20180603,gnomad211_exome&quot;, &quot;mutation_calling.annovar_operation&quot;: &quot;g,f,f,f,f,f&quot; } Loading… "],["parallelization-via-arrays.html", "Chapter 6 Parallelization via Arrays 6.1 The array type 6.2 Scattered tasks 6.3 Making our workflow run on multiple samples at once using scattered tasks and arrays 6.4 Referencing an array in a task 6.5 The workflow so far", " Chapter 6 Parallelization via Arrays We have a workflow that runs on a single sample. What if we want to process multiple samples at once? Let’s look at the various ways we can run our workflow more efficiently, as well as processing many samples in parallel. This is where WDL really shines. In this chapter, we’ll be going over: How to use scattered tasks to run a workflow on multiple samples at once How to use arrays effectively How to reference arrays in a task’s command section How arrays differ from Structs 6.1 The array type Arrays are essentially lists of another primitive type. It is most common to see Array[File] in WDLs, but an array can contain integers, floats, strings, and the like. An array can only have one of a given primitive type. For example, an Array[String] could contain the strings “cat” and “dog” but not the integer 1965 (however, it could have “1965” as a string). In chapter 5, we went over the struct data type and used it to handle a myriad of reference genome files. Arrays differ from structs in that arrays are numerically indexed, which means that a member of the array can be accessed by its position in the array. On the other hand, each variable within a struct has its own name, and you use that name to reference it rather than a numerical index. In WDL, arrays are 0 indexed, so the “first” value in an array is referenced by [0]. As per the WDL spec, arrays retain their order and are immutable – if you explicitly define an Array[String] with the members [“foo”, “bar”, “bizz”], you can be confident that “foo” will always be at index 0. Array[String] foobarbizz = [&quot;foo&quot;, &quot;bar&quot;, &quot;bizz&quot;] String foo = foobarbizz[0] # will always be &quot;foo&quot; Because arrays are immutable in WDL, if you wish to add values to an array, you will need to define a new array. You can combine an existing array and new values using flatten(), a WDL built-in function that will turned a nested array into a “flat” array, like so: Array[String] foobarbizz = [&quot;foo&quot;, &quot;bar&quot;, &quot;bizz&quot;] String foo = foobarbizz[0] # will always be &quot;foo&quot; Array[Array[String]] foobarbizz_but_with_bonus_foo = [foobarbizz, foo, foo, foo] # [[&quot;foo&quot;, &quot;bar&quot;, &quot;bizz&quot;], &quot;foo&quot;, &quot;foo&quot;, &quot;foo&quot;] Array[String] foobarbizzfoofoofoo = flatten(foobarbizz_but_with_bonus_foo) # [&quot;foo&quot;, &quot;bar&quot;, &quot;bizz&quot;, &quot;foo&quot;, &quot;foo&quot;, &quot;foo&quot;] 6.2 Scattered tasks Scattered tasks allow us to run a WDL task in parallel. This is especially useful on highly scalable backends such as HPCs or the cloud, as it allows us to potentially run hundreds or even thousands of instances of a task at the same time. The most common use case for this is processing many samples at the same time, but it can also be used for processing a single sample’s chromosomes in parallel, or similar situations where breaking up data into discrete “chunks” makes sense. It should be noted that a scattered task does not work the same way as multithreading, nor does it correlate with the cpu WDL runtime attribute. Every instance of a scattered task takes place in a new Docker image, and is essentially “unaware” of all other instances of that scattered task, with one exception: If an instance of a scattered task errors out, a WDL executor may attempt to shut down other ongoing instances of that scattered task. 6.2.1 Troubleshooting Scattered tasks are relatively simple in theory, but the way they interact with optional types can be unintuitive. As a general rule, you should avoid using optional types as the input of a scattered task whenever possible. Generally speaking, a WDL executor will try to run as many instances of a scattered task as it thinks your backend’s hardware can handle at once. Sometimes the WDL executor will overestimate what the backend is capable of and run too many instances of a scattered task at once. This almost never happens on scalable cloud-based backends such as Terra, but isn’t uncommon when running scattered tasks on a local machine. 6.3 Making our workflow run on multiple samples at once using scattered tasks and arrays When we originally wrote our workflow, we designed it to only run on a single sample at a time. However, we’ll likely want to run this workflow on multiple samples at the same time. For some workflows, this is a great way to directly compare samples to each other, but for our purposes we simply want to avoid running a workflow 100 times if we can instead run one workflow that handles 100 samples at once. For starters, we’ll want to change our workflow-level sample variables from File to Array[File]. However, we don’t need to change any of the reference genome files, because every instance of our tasks will be taking in the same reference genome files. In other words, our struct is unchanged. version 1.0 struct referenceGenome { File ref_fasta File ref_fasta_index File ref_dict File ref_amb File ref_ann File ref_bwt File ref_pac File ref_sa String ref_name } workflow mutation_calling { input { Array[File] tumorSamples referenceGenome refGenome File dbSNP_vcf File dbSNP_vcf_index File known_indels_sites_VCFs File known_indels_sites_indices ... } ... } Next, we will want to look at our chain of tasks. Each of these tasks are designed to take in a single sample. In theory, we could rewrite each task to iterate through an array of multiple samples. However, it’s much simpler to keep those tasks as single-sample tasks, and simply run them on one sample at a time. To do this, we encapsulate the task calls in the workflow document with scatter. scatter (tumorFastq in tumorSamples) { call BwaMem as tumorBwaMem { input: input_fastq = tumorFastq, refGenome = refGenome } call MarkDuplicates as tumorMarkDuplicates { input: input_bam = tumorBwaMem.analysisReadySorted } call ApplyBaseRecalibrator as tumorApplyBaseRecalibrator{ input: input_bam = tumorMarkDuplicates.markDuplicates_bam, input_bam_index = tumorMarkDuplicates.markDuplicates_bai, dbSNP_vcf = dbSNP_vcf, dbSNP_vcf_index = dbSNP_vcf_index, known_indels_sites_VCFs = known_indels_sites_VCFs, known_indels_sites_indices = known_indels_sites_indices, refGenome = refGenome } } A scatter is essentially the WDL version of a for loop. Every task within that loop will have access to a single File within the Array[File] that it is looping through. Within the scatter, downstream tasks can access outputs of upstream tasks like normal. So, the first tumor fastq file will go through BwaMem, then the resulting bam will go through MarkDuplicates, and the marked bam will undergo base recalibration. Since all three of these tasks are within the same scatter, each task only “sees” one sample at a time. However, outside the scatter, every task is considered in the context of all samples, so every output of those scattered tasks becomes arrays. As a result, our workflow-level outputs are now Array[File] instead of just File. output { Array[File] tumoralignedBamSorted = tumorBwaMem.analysisReadySorted Array[File] tumorMarkDuplicates_bam = tumorMarkDuplicates.markDuplicates_bam Array[File] tumorMarkDuplicates_bai = tumorMarkDuplicates.markDuplicates_bai Array[File] tumoranalysisReadyBam = tumorApplyBaseRecalibrator.recalibrated_bam Array[File] tumoranalysisReadyIndex = tumorApplyBaseRecalibrator.recalibrated_bai } You can reference a full copy of this workflow at the end of this chapter. 6.4 Referencing an array in a task In our example, each task only takes in one sample, so we are not directly inputting arrays into a file. However, it’s important to know how to input arrays in a task’s command section. If a task’s input variable is an array, we must include an array separator. In WDL 1.0, this is done using the sep= expression placeholder. Every value in the WDL Array[String] will be separated by whatever value is declared via sep. In this example, that is a simple space, as that is one way how to construct a bash variable. task count_words { input { Array[String] a_big_sentence } command &lt;&lt;&lt; ARRAY_OF_WORDS=(~{sep=&quot; &quot; a_big_sentence}) echo ${#ARRAY_OF_FILES[@]} &gt;&gt; length.txt # Note how the bash array uses ${} syntax, which could quickly get # confusing if we used that syntax for our WDL variables. This is # why we recommend using tilde + {} for your WDL variables. &gt;&gt;&gt; } It’s usually unnecessary to declare an Array[String], because a single String can have many words in it. That being said, an Array[String] can sometimes come in handy if it is made up of outputs from other tasks. We’ll talk more about chaining tasks together in upcoming chapters. The WDL 1.1 spec added a new built-in function, sep(), which replaces the sep= expression placeholder for arrays. This same version of the spec also notes that the sep= expression placeholder are deprecated and will be removed from future versions of WDL. For the time being, we recommend sticking with sep= as it is compatible with both WDL 1.0 and WDL 1.1, even though it is technically deprecated in WDL 1.1. If you’re not used to working in bash, the syntax for interacting with bash arrays can be unintuitive, but you don’t have to write a WDL’s command section only using bash. In fact, working in another language besides bash within a WDL can be a great way to write code quickly, or perform tasks that are more advanced than what a typical bash script can handle. Just be sure to set sep properly to ensure that your array is interpreted correctly. In this example, we place quotation marks before and after the variable to ensure that the first and last value of the list are given beginning and ending quotation marks respectively. task count_words_python { input { Array[String] a_big_sentence } command &lt;&lt;&lt; python &lt;&lt; CODE sentence = [ &quot;~{sep=&#39;&quot;, &quot;&#39; a_big_sentence}&quot; ] print(len(sentence)) CODE &gt;&gt;&gt; runtime { docker: &quot;python:latest&quot; } } 6.5 The workflow so far Let’s take another look at our workflow. The JSON metadata: { &quot;mutation_calling.tumorSamples&quot;: [&quot;/path/to/Tumor_1_KRAS_CALU1_combined_final.fastq&quot;, &quot;/path/to/Tumor_2_EGFR_HCC4006_combined.fastq&quot;], &quot;mutation_calling.refGenome&quot;: { &quot;ref_fasta&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta&quot;, &quot;ref_fasta_index&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.fai&quot;, &quot;ref_dict&quot;: &quot;/path/to/Homo_sapiens_assembly19.dict&quot;, &quot;ref_pac&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.pac&quot;, &quot;ref_sa&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.sa&quot;, &quot;ref_amb&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.amb&quot;, &quot;ref_ann&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.ann&quot;, &quot;ref_bwt&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.bwt&quot;, &quot;ref_name&quot;: &quot;hg19&quot; }, &quot;mutation_calling.dbSNP_vcf_index&quot;: &quot;/path/to/dbsnp_138.b37.vcf.gz.tbi&quot;, &quot;mutation_calling.dbSNP_vcf&quot;: &quot;/path/to/dbsnp_138.b37.vcf.gz&quot;, &quot;mutation_calling.known_indels_sites_indices&quot;: &quot;/path/to/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.idx&quot;, &quot;mutation_calling.known_indels_sites_VCFs&quot;: &quot;/path/to/Mills_and_1000G_gold_standard.indels.b37.sites.vcf&quot;, &quot;mutation_calling.af_only_gnomad&quot;: &quot;/path/to/af-only-gnomad.raw.sites.b37.vcf.gz&quot;, &quot;mutation_calling.af_only_gnomad_index&quot;: &quot;/path/to/af-only-gnomad.raw.sites.b37.vcf.gz.tbi&quot;, &quot;mutation_calling.annovar_protocols&quot;: &quot;refGene,knownGene,cosmic70,esp6500siv2_all,clinvar_20180603,gnomad211_exome&quot;, &quot;mutation_calling.annovar_operation&quot;: &quot;g,f,f,f,f,f&quot; } The JSON using the Fred Hutch HPC { &quot;mutation_calling.tumorSamples&quot;: [&quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/workflow_testing_data/WDL/wdl_101/HCC4006_final.fastq&quot;, &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/workflow_testing_data/WDL/wdl_101/CALU1_combined_final.fastq&quot;], &quot;mutation_calling.refGenome&quot;: { &quot;ref_fasta&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta&quot;, &quot;ref_fasta_index&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.fai&quot;, &quot;ref_dict&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.dict&quot;, &quot;ref_pac&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.pac&quot;, &quot;ref_sa&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.sa&quot;, &quot;ref_amb&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.amb&quot;, &quot;ref_ann&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.ann&quot;, &quot;ref_bwt&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.bwt&quot;, &quot;ref_name&quot;: &quot;hg19&quot; }, &quot;mutation_calling.dbSNP_vcf_index&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/dbsnp_138.b37.vcf.gz.tbi&quot;, &quot;mutation_calling.dbSNP_vcf&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/dbsnp_138.b37.vcf.gz&quot;, &quot;mutation_calling.known_indels_sites_indices&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.idx&quot;, &quot;mutation_calling.known_indels_sites_VCFs&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Mills_and_1000G_gold_standard.indels.b37.sites.vcf&quot;, &quot;mutation_calling.af_only_gnomad&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/af-only-gnomad.raw.sites.b37.vcf.gz&quot;, &quot;mutation_calling.af_only_gnomad_index&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/af-only-gnomad.raw.sites.b37.vcf.gz.tbi&quot;, &quot;mutation_calling.annovar_protocols&quot;: &quot;refGene,knownGene,cosmic70,esp6500siv2_all,clinvar_20180603,gnomad211_exome&quot;, &quot;mutation_calling.annovar_operation&quot;: &quot;g,f,f,f,f,f&quot; } Loading… "],["task-aliasing.html", "Chapter 7 Task Aliasing 7.1 Aliasing your first task 7.2 Aliasing other tasks 7.3 Paired tumor normal calling", " Chapter 7 Task Aliasing Right now, we have developed our workflow for only tumor samples of a patient, but our mutation caller Mutect2 performs best when each tumor is matched with a normal sample. Therefore, in order to make best use of Mutect2, we want to run a similar analysis on the normal samples as well, using tasks such as BwaMem, MarkDuplicates, and ApplyBaseRecalibrator. We could write new tasks for the normal samples, but they are run exactly the same as the tumor samples: we would like to reuse our existing tasks for these tumor samples. WDL has a feature that allows you to reuse the same task repeatedly through your workflow: task aliasing. Task aliasing allows for the re-use of task definitions within the same workflow under different names, or “aliases”. For example, our task for alignment, “BwaMem”, can be aliased for both tumor and normal samples. This way, you don’t need to copy and paste the same task definition multiple times, and when you modify the the task definition, all aliases will be updated. 7.1 Aliasing your first task We first demonstrate task aliasing for tumor-normal matched somatic mutation calling in which we use the same normal sample for both tumor samples. This implies that we have taken multiple tumor samples from the same patient, and we’re comparing all of them against a single normal sample. This is easy to write, but not a very common analysis. After this example, we follow up with a more sophisticated tumor-normal matched somatic mutation calling in which each patient has unique paired tumor-normal samples. This is the popular way of calling somatic mutation. You can only alias a task that is already defined, so we will start with aliasing the BwaMem task for normal samples rather than writing a new BwaMem task specifically for normal samples. We want to do this so it can run this task on the “normal” samples and store them separately. First, make sure that in your workflow input, you reference to the normal sample as input. workflow mutation_calling { input { ... File normalFastq ... } Then, you need to call the task you want to alias and use as to the alias_of_your_choice. Within the task, you need to make sure that all the inputs reflect the new variable, so input_fastq is directed to normalFastq. call BwaMem as normalBwaMem { input: input_fastq = normalFastq, refGenome = refGenome } In the output of the workflow, you also want to make sure that in you are saving the appropriate outputs to reflect the task alias. output { File normalalignedBamSorted = normalBwaMem.analysisReadySorted } 7.2 Aliasing other tasks We can do this for the other two tasks in our workflow as well for the normal sample: call MarkDuplicates as normalMarkDuplicates { input: input_bam = normalBwaMem.analysisReadySorted } call ApplyBaseRecalibrator as normalApplyBaseRecalibrator { input: input_bam = normalMarkDuplicates.markDuplicates_bam, input_bam_index = normalMarkDuplicates.markDuplicates_bai, dbSNP_vcf = dbSNP_vcf, dbSNP_vcf_index = dbSNP_vcf_index, known_indels_sites_VCFs = known_indels_sites_VCFs, known_indels_sites_indices = known_indels_sites_indices, refGenome = refGenome } After adding these steps to the workflow, we will have our normal sample aligned and recalibrated. Together with the tumor sample, we can use the paired version of Mutect2 via the new Mutect2Paired task. The workflow so far using the same normal sample for tumor-normal mutation calling. version 1.0 ## WDL 101 example workflow ## ## This WDL workflow is intended to be used along with the WDL 101 docs. ## This workflow should be used for inspiration purposes only. ## ## We use three samples ## Samples: ## MOLM13: Normal sample ## CALU1: KRAS G12C mutant ## HCC4006: EGFR Ex19 deletion mutant ## ## Input requirements: ## - combined fastq files for chromosome 12 and 7 +/- 200bp around the sites of mutation only ## ## Output Files: ## - An aligned bam for all 3 samples (with duplicates marked and base quality recalibrated) ## ## Workflow developed by Sitapriya Moorthi, Chris Lo and Taylor Firman @ Fred Hutch and Ash (Aisling) O&#39;Farrell @ UCSC LMD: 02/28/24 for use @ Fred Hutch. struct referenceGenome { File ref_fasta File ref_fasta_index File ref_dict File ref_amb File ref_ann File ref_bwt File ref_pac File ref_sa String ref_name } workflow mutation_calling { input { Array[File] tumorSamples File normalFastq referenceGenome refGenome # Files for specific tools File dbSNP_vcf File dbSNP_vcf_index File known_indels_sites_VCFs File known_indels_sites_indices File af_only_gnomad File af_only_gnomad_index # Annovar options String annovar_protocols String annovar_operation } # First, process the non-tumor normal sample call BwaMem as normalBwaMem { input: input_fastq = normalFastq, refGenome = refGenome } call MarkDuplicates as normalMarkDuplicates { input: input_bam = normalBwaMem.analysisReadySorted } call ApplyBaseRecalibrator as normalApplyBaseRecalibrator { input: input_bam = normalMarkDuplicates.markDuplicates_bam, input_bam_index = normalMarkDuplicates.markDuplicates_bai, dbSNP_vcf = dbSNP_vcf, dbSNP_vcf_index = dbSNP_vcf_index, known_indels_sites_VCFs = known_indels_sites_VCFs, known_indels_sites_indices = known_indels_sites_indices, refGenome = refGenome } # Scatter for &quot;tumor&quot; samples scatter (tumorSample in tumorSamples) { call BwaMem as tumorBwaMem { input: input_fastq = tumorSample, refGenome = refGenome } call MarkDuplicates as tumorMarkDuplicates { input: input_bam = tumorBwaMem.analysisReadySorted } call ApplyBaseRecalibrator as tumorApplyBaseRecalibrator{ input: input_bam = tumorMarkDuplicates.markDuplicates_bam, input_bam_index = tumorMarkDuplicates.markDuplicates_bai, dbSNP_vcf = dbSNP_vcf, dbSNP_vcf_index = dbSNP_vcf_index, known_indels_sites_VCFs = known_indels_sites_VCFs, known_indels_sites_indices = known_indels_sites_indices, refGenome = refGenome } call Mutect2Paired { input: tumor_bam = tumorApplyBaseRecalibrator.recalibrated_bam, tumor_bam_index = tumorApplyBaseRecalibrator.recalibrated_bai, normal_bam = normalApplyBaseRecalibrator.recalibrated_bam, normal_bam_index = normalApplyBaseRecalibrator.recalibrated_bai, refGenome = refGenome, genomeReference = af_only_gnomad, genomeReferenceIndex = af_only_gnomad_index } call annovar { input: input_vcf = Mutect2Paired.output_vcf, ref_name = refGenome.ref_name, annovar_operation = annovar_operation, annovar_protocols = annovar_protocols } } output { Array[File] tumoralignedBamSorted = tumorBwaMem.analysisReadySorted Array[File] tumorMarkDuplicates_bam = tumorMarkDuplicates.markDuplicates_bam Array[File] tumorMarkDuplicates_bai = tumorMarkDuplicates.markDuplicates_bai Array[File] tumoranalysisReadyBam = tumorApplyBaseRecalibrator.recalibrated_bam Array[File] tumoranalysisReadyIndex = tumorApplyBaseRecalibrator.recalibrated_bai File normalalignedBamSorted = normalBwaMem.analysisReadySorted File normalmarkDuplicates_bam = normalMarkDuplicates.markDuplicates_bam File normalmarkDuplicates_bai = normalMarkDuplicates.markDuplicates_bai File normalanalysisReadyBam = normalApplyBaseRecalibrator.recalibrated_bam File normalanalysisReadyIndex = normalApplyBaseRecalibrator.recalibrated_bai Array[File] Mutect2Paired_Vcf = Mutect2Paired.output_vcf Array[File] Mutect2Paired_VcfIndex = Mutect2Paired.output_vcf_index Array[File] Mutect2Paired_AnnotatedVcf = annovar.output_annotated_vcf Array[File] Mutect2Paired_AnnotatedTable = annovar.output_annotated_table } parameter_meta { tumorSamples: &quot;Tumor .fastq, one sample per .fastq file (expects Illumina)&quot; normalFastq: &quot;Non-tumor .fastq (expects Illumina)&quot; dbSNP_vcf: &quot;dbSNP VCF for mutation calling&quot; dbSNP_vcf_index: &quot;dbSNP VCF index&quot; known_indels_sites_VCFs: &quot;Known indel site VCF for mutation calling&quot; known_indels_sites_indices: &quot;Known indel site VCF indicies&quot; af_only_gnomad: &quot;gnomAD population allele fraction for mutation calling&quot; af_only_gnomad_index: &quot;gnomAD population allele fraction index&quot; annovar_protocols: &quot;annovar protocols: see https://annovar.openbioinformatics.org/en/latest/user-guide/startup&quot; annovar_operation: &quot;annovar operation: see https://annovar.openbioinformatics.org/en/latest/user-guide/startup&quot; } } #################### # Task definitions # #################### # Align fastq file to the reference genome task BwaMem { input { File input_fastq referenceGenome refGenome } String base_file_name = basename(input_fastq, &quot;.fastq&quot;) String ref_fasta_local = basename(refGenome.ref_fasta) String read_group_id = &quot;ID:&quot; + base_file_name String sample_name = &quot;SM:&quot; + base_file_name String platform_info = &quot;PL:illumina&quot; command &lt;&lt;&lt; set -eo pipefail mv &quot;~{refGenome.ref_fasta}&quot; . mv &quot;~{refGenome.ref_fasta_index}&quot; . mv &quot;~{refGenome.ref_dict}&quot; . mv &quot;~{refGenome.ref_amb}&quot; . mv &quot;~{refGenome.ref_ann}&quot; . mv &quot;~{refGenome.ref_bwt}&quot; . mv &quot;~{refGenome.ref_pac}&quot; . mv &quot;~{refGenome.ref_sa}&quot; . bwa mem \\ -p -v 3 -t 16 -M -R &#39;@RG\\t~{read_group_id}\\t~{sample_name}\\t~{platform_info}&#39; \\ &quot;~{ref_fasta_local}&quot; &quot;~{input_fastq}&quot; &gt; &quot;~{base_file_name}.sam&quot; samtools view -1bS -@ 15 -o &quot;~{base_file_name}.aligned.bam&quot; &quot;~{base_file_name}.sam&quot; samtools sort -@ 15 -o &quot;~{base_file_name}.sorted_query_aligned.bam&quot; &quot;~{base_file_name}.aligned.bam&quot; &gt;&gt;&gt; output { File analysisReadySorted = &quot;~{base_file_name}.sorted_query_aligned.bam&quot; } runtime { memory: &quot;48 GB&quot; cpu: 16 docker: &quot;ghcr.io/getwilds/bwa:0.7.17&quot; } } # Mark duplicates on a BAM file task MarkDuplicates { input { File input_bam } String base_file_name = basename(input_bam, &quot;.sorted_query_aligned.bam&quot;) command &lt;&lt;&lt; gatk MarkDuplicates \\ --INPUT &quot;~{input_bam}&quot; \\ --OUTPUT &quot;~{base_file_name}.duplicates_marked.bam&quot; \\ --METRICS_FILE &quot;~{base_file_name}.duplicate_metrics&quot; \\ --CREATE_INDEX true \\ --OPTICAL_DUPLICATE_PIXEL_DISTANCE 100 \\ --VALIDATION_STRINGENCY SILENT &gt;&gt;&gt; runtime { docker: &quot;ghcr.io/getwilds/gatk:4.3.0.0&quot; memory: &quot;48 GB&quot; cpu: 4 } output { File markDuplicates_bam = &quot;~{base_file_name}.duplicates_marked.bam&quot; File markDuplicates_bai = &quot;~{base_file_name}.duplicates_marked.bai&quot; File duplicate_metrics = &quot;~{base_file_name}.duplicates_marked.bai&quot; } } # Base quality recalibration task ApplyBaseRecalibrator { input { File input_bam File input_bam_index File dbSNP_vcf File dbSNP_vcf_index File known_indels_sites_VCFs File known_indels_sites_indices referenceGenome refGenome } String base_file_name = basename(input_bam, &quot;.duplicates_marked.bam&quot;) String ref_fasta_local = basename(refGenome.ref_fasta) String dbSNP_vcf_local = basename(dbSNP_vcf) String known_indels_sites_VCFs_local = basename(known_indels_sites_VCFs) command &lt;&lt;&lt; set -eo pipefail mv &quot;~{refGenome.ref_fasta}&quot; . mv &quot;~{refGenome.ref_fasta_index}&quot; . mv &quot;~{refGenome.ref_dict}&quot; . mv &quot;~{dbSNP_vcf}&quot; . mv &quot;~{dbSNP_vcf_index}&quot; . mv &quot;~{known_indels_sites_VCFs}&quot; . mv &quot;~{known_indels_sites_indices}&quot; . samtools index &quot;~{input_bam}&quot; gatk --java-options &quot;-Xms8g&quot; \\ BaseRecalibrator \\ -R &quot;~{ref_fasta_local}&quot; \\ -I &quot;~{input_bam}&quot; \\ -O &quot;~{base_file_name}.recal_data.csv&quot; \\ --known-sites &quot;~{dbSNP_vcf_local}&quot; \\ --known-sites &quot;~{known_indels_sites_VCFs_local}&quot; \\ gatk --java-options &quot;-Xms8g&quot; \\ ApplyBQSR \\ -bqsr &quot;~{base_file_name}.recal_data.csv&quot; \\ -I &quot;~{input_bam}&quot; \\ -O &quot;~{base_file_name}.recal.bam&quot; \\ -R &quot;~{ref_fasta_local}&quot; \\ # finds the current sort order of this bam file samtools view -H &quot;~{base_file_name}.recal.bam&quot; | grep @SQ | sed &#39;s/@SQ\\tSN:\\|LN://g&#39; &gt; &quot;~{base_file_name}.sortOrder.txt&quot; &gt;&gt;&gt; output { File recalibrated_bam = &quot;~{base_file_name}.recal.bam&quot; File recalibrated_bai = &quot;~{base_file_name}.recal.bai&quot; File sortOrder = &quot;~{base_file_name}.sortOrder.txt&quot; } runtime { memory: &quot;36 GB&quot; cpu: 2 docker: &quot;ghcr.io/getwilds/gatk:4.3.0.0&quot; } } # Variant calling via mutect2 (tumor-and-normal mode) task Mutect2Paired { input { File tumor_bam File tumor_bam_index File normal_bam File normal_bam_index referenceGenome refGenome File genomeReference File genomeReferenceIndex } String base_file_name_tumor = basename(tumor_bam, &quot;.recal.bam&quot;) String ref_fasta_local = basename(refGenome.ref_fasta) String genomeReference_local = basename(genomeReference) command &lt;&lt;&lt; set -eo pipefail mv &quot;~{refGenome.ref_fasta}&quot; . mv &quot;~{refGenome.ref_fasta_index}&quot; . mv &quot;~{refGenome.ref_dict}&quot; . mv &quot;~{genomeReference}&quot; . mv &quot;~{genomeReferenceIndex}&quot; . gatk --java-options &quot;-Xms16g&quot; Mutect2 \\ -R &quot;~{ref_fasta_local}&quot; \\ -I &quot;~{tumor_bam}&quot; \\ -I &quot;~{normal_bam}&quot; \\ -O preliminary.vcf.gz \\ --germline-resource &quot;~{genomeReference_local}&quot; \\ gatk --java-options &quot;-Xms16g&quot; FilterMutectCalls \\ -V preliminary.vcf.gz \\ -O &quot;~{base_file_name_tumor}.mutect2.vcf.gz&quot; \\ -R &quot;~{ref_fasta_local}&quot; \\ --stats preliminary.vcf.gz.stats \\ &gt;&gt;&gt; runtime { docker: &quot;ghcr.io/getwilds/gatk:4.3.0.0&quot; memory: &quot;24 GB&quot; cpu: 1 } output { File output_vcf = &quot;${base_file_name_tumor}.mutect2.vcf.gz&quot; File output_vcf_index = &quot;${base_file_name_tumor}.mutect2.vcf.gz.tbi&quot; } } # Annotate VCF using annovar task annovar { input { File input_vcf String ref_name String annovar_protocols String annovar_operation } String base_vcf_name = basename(input_vcf, &quot;.vcf.gz&quot;) command &lt;&lt;&lt; set -eo pipefail perl /annovar/table_annovar.pl &quot;~{input_vcf}&quot; /annovar/humandb/ \\ -buildver &quot;~{ref_name}&quot; \\ -outfile &quot;~{base_vcf_name}&quot; \\ -remove \\ -protocol &quot;~{annovar_protocols}&quot; \\ -operation &quot;~{annovar_operation}&quot; \\ -nastring . -vcfinput &gt;&gt;&gt; runtime { docker : &quot;ghcr.io/getwilds/annovar:${ref_name}&quot; cpu: 1 memory: &quot;2GB&quot; } output { File output_annotated_vcf = &quot;~{base_vcf_name}.${ref_name}_multianno.vcf&quot; File output_annotated_table = &quot;~{base_vcf_name}.${ref_name}_multianno.txt&quot; } } 7.3 Paired tumor normal calling Now that we are comfortable with task aliasing, we follow up with a more sophisticated tumor-normal matched somatic mutation calling in which each patient has unique paired tumor-normal samples. In order to do so, we need to ensure that each patient has an unique tumor and normal sample. We could modify our workflow inputs to be: workflow mutation_calling { input { Array[File] tumorSamples Array[file] normalSamples ... } } but that would be a bit awkward to scatter through, as we would need to create a separate index to keep track of which sample we are using in the array of tumorSamples and normalSamples. Instead, we write a struct for our paired samples: struct pairedSample { File tumorSample File normalSample } so that in our workflow inputs, we use an array of pairedSample: workflow mutation_calling { input { Array[pairedSample] samples ... } } and we can scatter on samples. Within our scatter, we use task aliasing to call tumor-specific and normal-specific tasks until Mutect2Paired and annovar. scatter (sample in samples) { #Tumors call BwaMem as tumorBwaMem { input: input_fastq = sample.tumorSample, refGenome = refGenome } #Normals call BwaMem as normalBwaMem { input: input_fastq = sample.normalSample, refGenome = refGenome } ... } Putting it together: The JSON metadata: { &quot;mutation_calling.samples&quot;: [{&quot;tumorSample&quot;: &quot;/path/to/Tumor_1_KRAS_CALU1_combined_final.fastq&quot;, &quot;normalSample&quot;: &quot;/path/to/Normal_1_MOLM13_combined_final.fastq&quot;}, {&quot;tumorSample&quot;: &quot;/path/to/Tumor_2_EGFR_HCC4006_combined.fastq&quot;, &quot;normalSample&quot;: &quot;/path/to/Normal_2_MOLM13_combined_final.fastq&quot;}], &quot;mutation_calling.refGenome&quot;: { &quot;ref_fasta&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta&quot;, &quot;ref_fasta_index&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.fai&quot;, &quot;ref_dict&quot;: &quot;/path/to/Homo_sapiens_assembly19.dict&quot;, &quot;ref_pac&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.pac&quot;, &quot;ref_sa&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.sa&quot;, &quot;ref_amb&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.amb&quot;, &quot;ref_ann&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.ann&quot;, &quot;ref_bwt&quot;: &quot;/path/to/Homo_sapiens_assembly19.fasta.bwt&quot;, &quot;ref_name&quot;: &quot;hg19&quot; }, &quot;mutation_calling.dbSNP_vcf_index&quot;: &quot;/path/to/dbsnp_138.b37.vcf.gz.tbi&quot;, &quot;mutation_calling.dbSNP_vcf&quot;: &quot;/path/to/dbsnp_138.b37.vcf.gz&quot;, &quot;mutation_calling.known_indels_sites_indices&quot;: &quot;/path/to/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.idx&quot;, &quot;mutation_calling.known_indels_sites_VCFs&quot;: &quot;/path/to/Mills_and_1000G_gold_standard.indels.b37.sites.vcf&quot;, &quot;mutation_calling.af_only_gnomad&quot;: &quot;/path/to/af-only-gnomad.raw.sites.b37.vcf.gz&quot;, &quot;mutation_calling.af_only_gnomad_index&quot;: &quot;/path/to/af-only-gnomad.raw.sites.b37.vcf.gz.tbi&quot;, &quot;mutation_calling.annovar_protocols&quot;: &quot;refGene,knownGene,cosmic70,esp6500siv2_all,clinvar_20180603,gnomad211_exome&quot;, &quot;mutation_calling.annovar_operation&quot;: &quot;g,f,f,f,f,f&quot; } The JSON using the Fred Hutch HPC { &quot;mutation_calling.sampleFastq&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/workflow_testing_data/WDL/wdl_101/HCC4006_final.fastq&quot;, &quot;mutation_calling.ref_fasta&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta&quot;, &quot;mutation_calling.ref_fasta_index&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.fai&quot;, &quot;mutation_calling.ref_dict&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.dict&quot;, &quot;mutation_calling.ref_pac&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.pac&quot;, &quot;mutation_calling.ref_sa&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.sa&quot;, &quot;mutation_calling.ref_amb&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.amb&quot;, &quot;mutation_calling.ref_ann&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.ann&quot;, &quot;mutation_calling.ref_bwt&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Homo_sapiens_assembly19.fasta.bwt&quot;, &quot;mutation_calling.ref_name&quot;: &quot;hg19&quot;, &quot;mutation_calling.dbSNP_vcf_index&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/dbsnp_138.b37.vcf.gz.tbi&quot;, &quot;mutation_calling.dbSNP_vcf&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/dbsnp_138.b37.vcf.gz&quot;, &quot;mutation_calling.known_indels_sites_indices&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Mills_and_1000G_gold_standard.indels.b37.sites.vcf.idx&quot;, &quot;mutation_calling.known_indels_sites_VCFs&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/Mills_and_1000G_gold_standard.indels.b37.sites.vcf&quot;, &quot;mutation_calling.af_only_gnomad&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/af-only-gnomad.raw.sites.b37.vcf.gz&quot;, &quot;mutation_calling.af_only_gnomad_index&quot;: &quot;/fh/fast/paguirigan_a/pub/ReferenceDataSets/genome_data/human/hg19/af-only-gnomad.raw.sites.b37.vcf.gz.tbi&quot;, &quot;mutation_calling.annovar_protocols&quot;: &quot;refGene,knownGene,cosmic70,esp6500siv2_all,clinvar_20180603,gnomad211_exome&quot;, &quot;mutation_calling.annovar_operation&quot;: &quot;g,f,f,f,f,f&quot; } Now we have a complete workflow that reflects our original plan in Defining a workflow plan! Loading… "],["optional-types.html", "Chapter 8 Optional types 8.1 Optional inputs 8.2 Optional outputs 8.3 The final workflow", " Chapter 8 Optional types WDL supports defining optional variables, which are denoted with a question mark next to the type declaration. These variables may or may not defined. Optional types are powerful, but they can quickly cause problems in complicated workflows. This is especially the case when working with scattered tasks. In this section, we’ll go over how optional variables can be used, and where they can cause problems. 8.1 Optional inputs The most common use case for optional variables are optional inputs for the user to provide some sort of file, for example, a reference genome or a metadata file. Sometimes, you will want optional inputs to fall back on a known file or value. A good place to use optional types in our workflow would be in the bwa mem task. bwa mem on default settings may attempt to use too many threads, which could cause this task to fail if it ran on weaker hardware such as a laptop. In order to allow the user to adjust how many threads bwa mem uses, and by extension make our WDL run on a wider variety of machines, we can edit the task to add a new variable. This new variable will replace -t 16 in our call to bwa mem. task BwaMem { input { File input_fastq referenceGenome refGenome # Number of threads to use - must be defined Int threads } command &lt;&lt;&lt; set -eo pipefail mv &quot;~{refGenome.ref_fasta}&quot; . mv &quot;~{refGenome.ref_fasta_index}&quot; . mv &quot;~{refGenome.ref_dict}&quot; . mv &quot;~{refGenome.ref_amb}&quot; . mv &quot;~{refGenome.ref_ann}&quot; . mv &quot;~{refGenome.ref_bwt}&quot; . mv &quot;~{refGenome.ref_pac}&quot; . mv &quot;~{refGenome.ref_sa}&quot; . bwa mem \\ -p -v 3 -t ~{threads} -M -R &#39;@RG\\t~{read_group_id}\\t~{sample_name}\\t~{platform_info}&#39; \\ &quot;~{ref_fasta_local}&quot; &quot;~{input_fastq}&quot; &gt; &quot;~{base_file_name}.sam&quot; samtools view -1bS -@ 15 -o &quot;~{base_file_name}.aligned.bam&quot; &quot;~{base_file_name}.sam&quot; samtools sort -@ 15 -o &quot;~{base_file_name}.sorted_query_aligned.bam&quot; &quot;~{base_file_name}.aligned.bam&quot; [...] } Because ~{threads} is an integer variable, it cannot have a space in it, so we don’t need to surround ~{threads} with quotation marks like we should String or File variables. Now we can set Int? bwa_mem_threads as a workflow-level optional input. Making it optional means that users who do not want to think about threads – perhaps they’re using an HPC or other powerful backend – do not have to worry about filling in that value. In the workflow body, when we call bwa mem, the value of bwa_mem_threads is passed to the BwaMem task. workflow mutation_calling { input { Array[File] tumorSamples File normalFastq referenceGenome refGenome # Optional variable for bwa mem Int? bwa_mem_threads # Files for specific tools File dbSNP_vcf File dbSNP_vcf_index File known_indels_sites_VCFs File known_indels_sites_indices File af_only_gnomad File af_only_gnomad_index # Annovar options String annovar_protocols String annovar_operation } call BwaMem as normalBwaMem { input: input_fastq = normalFastq, refGenome = refGenome, threads = bwa_mem_threads } [...] } But what happens if the workflow-level variable bwa_mem_threads isn’t defined? The task BwaMem will then get no value for the task-level variable threads. This will cause an error at runtime. We can’t solve this merely by making the task level variable optional too – otherwise, if threads isn’t defined, our bwa mem call will provide -t without a number after it, which is invalid. One way to address this is with the WDL built-in function select_first(), which takes in an array of values. In the workflow body, we can use this plus a fallback value (or another variable) to ensure that the BwaMem task always gets a valid integer for the threads argument. call BwaMem as normalBwaMem { input: input_fastq = normalFastq, refGenome = refGenome, threads = select_first([bwa_mem_threads, 16]) } This is a perfectly valid way of doing things, but the task itself still relies on that variable being defined. In the context of our workflow, that doesn’t matter. However, keep in mind it is a common practice to reuse code for multiple scripts – whether that is directly importing other WDL tasks using WDL’s importing feature, or simply copy-pasting boilerplate code. With that in mind, it’s a good idea to make tasks as “stand-alone” as possible, and not reliant on the workflow calling them to use select_first(). As such, it’s more common to make the task itself handle this by providing a fallback value, like so: task BwaMem { input { File input_fastq referenceGenome refGenome Int threads = 16 # if a workflow passes an optional variable with no value, fall back to 16 } [...] } Here’s another way of providing a fallback value for optional inputs within a task. In the following task, the phylogenetic tree program UShER expects a reference genome, an input phylogenetic tree, samples in the form of diff files, and an output file name. However, the Docker image we’re using (ashedpotatoes/usher-plus:0.0.2) already contains the H37Rv reference genome, which is the standard reference genome for Mycobacterium tuberculosis. If we are working with tuberculosis samples, there is no reason to provide a reference genome – we might as well use the one that’s already in the Docker image. task usher_sampled_diff { input { File diff File input_tree # in MAT format, extension should be .pb String output_file_name File? ref_genome } command &lt;&lt;&lt; if [[ &quot;~{ref_genome}&quot; = &quot;&quot; ]] then # if not defined, fall back to H37Rv in the Docker image ref=&quot;/HOME/usher/ref/Ref.H37Rv/ref.fa&quot; else ref=&quot;~{ref_genome}&quot; fi usher-sampled --diff &quot;~{diff}&quot; \\ -i &quot;${input_tree}&quot; \\ --ref &quot;$ref&quot; \\ -o &quot;~{output_file_name}&quot; &gt;&gt;&gt; runtime { docker: &quot;ashedpotatoes/usher-plus:0.0.2&quot; } output { File usher_tree = output_file_name } } 8.2 Optional outputs Optional outputs are usually created by explicitly declaring a task or workflow’s output variable to be optional. They can also be created by enclosing a task within an if-block, which will cause all outputs of that task to be considered optional. 8.2.1 Declaring a task’s output to be optional Most backends will save the task-level outputs of a task, even if those outputs are not workflow-level outputs. This can be very useful if you want to be able to check intermediate files, or perhaps log files for certain bioinformatic tools. However, if you’re not actively debugging, those files may take up a lot of space on your backend if you’re running on many samples over time. You could change the workflow every time you wish to keep those intermediate inputs, but it would be easier to make the output optional. task usher_sampled_diff { input { File diff File input_tree # in MAT format, extension should be .pb String output_file_name File? ref_genome Boolean detailed_clades } # UShER will output clade information if you pass the -D flag to it, but # we don&#39;t want users to have to type in flags as a String variable. Instead, # we just make a Boolean variable called detailed_clades, and outside the inputs # section, we write the string &quot;-D&quot; if detailed_clades is true, or an empty string # if detailed_clades is false. String detailed_clades_flag = if !(detailed_clades) then &quot;&quot; else &quot;-D &quot; command &lt;&lt;&lt; if [[ &quot;~{ref_genome}&quot; = &quot;&quot; ]] then ref=&quot;/HOME/usher/ref/Ref.H37Rv/ref.fa&quot; else ref=&quot;~{ref_genome}&quot; fi usher-sampled ~{detailed_clades_flag} \\ --diff &quot;~{diff}&quot; \\ -i &quot;${input_tree}&quot; \\ --ref &quot;$ref&quot; \\ -o &quot;~{output_file_name}&quot; &gt;&gt;&gt; runtime { docker: &quot;ashedpotatoes/usher-plus:0.0.2&quot; } output { File usher_tree = output_file_name File? clades = &quot;clades.txt&quot; # only if detailed_clades = true } } If we want, we can make this task-level output a workflow-level output – perhaps we use a backend that does not save task-level outputs, or it is simply easier to access workflow-level outputs on this particular backend. Provided that the task is not scattered, this is very easy to do. (We’ll talk about scattered tasks later, as they complicate matters.) In the workflow output sections, we simply define the output as optional like we did in the task itself. 8.2.2 Making an entire task optional Let’s say we want to make an entire task optional. Perhaps it provides additional QC information, or isn’t relevant to certain data types. Although WDL has a concept of if, it does not have any concept of else, so to make mutually exclusive tasks, you essentially need use two mutually exclusive if-statements. In the example below, we use two mutually exclusive if-statements to decide whether to run task_x or task_y. This is a valid way of doing things, and will work as expected: workflow do_x_or_y { input { Boolean do_x = true # if true do task_x, if false do task_y File some_input } if (do_x = true) { call task_x { input: some_input = some_input } } if (do_x = false) { call task_y { input: some_input = some_input } } } However, we need to keep in mind that WDL is not “aware” that these two tasks are mutually exclusive. You and I both know that if task_x has run, then task_y must not have run, but your WDL executor does not know this. So, as a consequence, you cannot use mutually exclusive if-blocks to set a variable to a particular value. # this workflow will fail miniwdl check or womtool workflow do_x_or_y { input { Boolean do_x = true # if true do task_x, if false do task_y File some_input } if (do_x = true) { Int some_variable = 10 # this is problematic, even though only one of these if blocks will execute! call task_x { input: some_input = some_input } } if (do_x = false) { Int some_variable = 5 # this is problematic, even though only one of these if blocks will execute! call task_y { input: some_input = some_input } } } Another consequence is that both of your mutually exclusive tasks’ outputs will be considered optional types, even though you can be certain that set of outputs absolutely do exist and one set of outputs absolutely do not exist. workflow do_x_or_y { input { Boolean do_x = true # if true do task_x, if false do task_y File some_input } if (do_x = true) { call task_x { input: some_input = some_input } } if (do_x = false) { call task_y { input: some_input = some_input } } outputs { # these both MUST be optional File? task_x_output = task_x.something File? task_y_output = task_y.something } } Checking if a variable is defined You can check if a variable is defined at runtime using the WDL built-in defined(), which returns a Boolean value. However, defined() can give unexpected output, as we will go over later. For the time being, let’s use a simple example where we set a Boolean variable via defined(). This will work as expected – if task_x ran and created the output task_x.something, then did_x_run will be true, and so on. workflow do_x_or_y { input { Boolean do_x = true # if true do task_x, if false do task_y File some_input } if (do_x = true) { call task_x { input: some_input = some_input } } if (do_x = false) { call task_y { input: some_input = some_input } } # these work as you would expect Boolean did_x_run = defined(task_x.something) Boolean did_y_run = defined(task_y.something) outputs { File? task_x_output = task_x.something File? task_y_output = task_y.something Boolean did_x_run = did_x_run Boolean did_y_run = did_y_run } } Note that although you could decide whether or not to execute a task by wrapping that task in a defined() if-block, you cannot use defined() to coerce a variable. It’s generally best to use select_first() for type coercion, although we will mention some exceptions when it comes to arrays. # if some_integer has type Int?, we cannot coerce it like this: if defined(some_integer) { Int real_some_integer = some_integer } # because outside of the if-block, real_some_integer is considered optional You should not use defined() with optional arrays, as it often acts in unexpected ways. It is very easy to accidentally create a defined() check that always returns true, as WDL executors may define “optional” arrays even if they have no values. For example, let’s consider task_x and task_y again. To recap, these are mutually exclusive tasks which each output a single File. If these tasks are scattered from outside their respective if-blocks, the their outputs are no longer File?, instead, they are arrays. But are they arrays of optional File?s, or are the arrays themselves optional? Let’s try to find out: workflow do_x_or_y { input { Boolean do_x = true # if true do task_x, if false do task_y Array[File] some_inputs } scatter(some_input in some_inputs) { if (do_x = true) { call task_x { input: some_input = some_input } } if (do_x = false) { call task_y { input: some_input = some_input } } } # these both always return true! Boolean did_x_run = defined(task_x.something) Boolean did_y_run = defined(task_y.something) outputs { File? task_x_output = task_x.something File? task_y_output = task_y.something Boolean did_x_run = did_x_run Boolean did_y_run = did_y_run } } Instead of using defined() on an array, consider simply checking its length. An empty array has a length of 0, so if the length of an array is greater than 1, you know that there is at least one item existing in that array. Working with optional arrays and advanced usage of select_first() Coercing arrays with optional components Optional arrays can be difficult to work with. One of the reasons for this is that they sometimes give unexpected values for defined(). As mentioned earlier, this can be somewhat alleviated by checking the length of an array rather than whether or not it is defined, but the problems do not end there: Attempting to scatter on an optional array can cause issues that are difficult to debug. It’s generally best to coerce optional arrays into not-optional arrays as much as possible. Thankfully, we can use select_all() to select only the defined elements of an array. For our example above, we can go even further by creating an array that will always have a defined value, whether or not do_x is true. We combine the power of select_all() and flatten() to create the following: Array[File] x_or_y_outfiles = flatten(select_all(x.outfile), select_all(y.outfile)) Make sure not to use flatten(select_all(x.outfile, y.outfile))! That would result in an Array[File] that has a null value, which can cause hard-to-diagnose issues later on in your pipeline. If you’re absolutely certain your variable, in this particular scope, is defined, you can make the second member of the select_first() array a random bogus fallback value. You may want to add a comment to prevent you or other programmers accidentally breaking things should they edit the code later. if defined(some_integer) { # needed to define real_some_integer, beware of changing this! Int real_some_integer = select_first([some_integer, 1975]) } 8.3 The final workflow At this point, we’ve completed our workflow. Here’s what it looks like now: Loading… "],["optimization.html", "Chapter 9 Optimization 9.1 Common optimizing/parallelizing methods 9.2 Scatter-Gather on chromosomes", " Chapter 9 Optimization In our last chapter, we will explore ways to optimize our completed workflow. Generally, in computing, we are working with the following finite resources: Computing time: a task can take milliseconds to days to complete. We find ways to reduce our computing time so that we have results sooner. Central Processing Unit (CPU) usage: a task requires at least one CPU to be the “brain” of the computer to run computation, move data around, and access the memory. Tasks can make use of multiple CPUs to process in parallel. Memory usage (RAM): a task needs a place to store data while computing on the job. Reducing the memory usage can allow the task to be more efficient. Disk usage: A task usually requires some input data and have an output, and needs disk storage to store the information when the task isn’t running. Changing usage of any of these resources often affect other resources. For instance, there’s often a tradeoff between computing time and memory usage. In order to be efficient with these finite resources, we will introduce some optimization methods that are common in building an efficient workflow, and provide an example on a simple optimization method, “Embarrassingly Parallel Scatter-Gather”. 9.1 Common optimizing/parallelizing methods 9.1.1 Memory optimization To optimize the amount of memory used for a task, you can profile the task to understand how much memory is being used and where the bottlenecks are. Most programming languages have a large selection of memory profilers to analyze the performance of your code, and they can be starting points to consider how to use less memory. When a bottleneck is found, you might consider to use a more efficient algorithm to reduce memory usage. Or, perhaps you might use more optimal data structures, such as databases or sparse data structures, that better fit the data type of your problem. Another common memory analysis is to understand how memory usage scales relative to the input for the task. For example, as you increase the VCF file size for a task, how much does the memory usage scale? Is there something you can do so that it scales with a smaller magnitude? A technique you may want to consider if your memory usage scales with your input data size is that you could break down the input into smaller parts, and run your task on these smaller parts, each requesting smaller amount of memory. This wouldn’t reduce the total amount of memory you use, but many computing backends prefer many tasks using small amounts of a memory instead of a single job requiring a large amount of memory. This would also help optimizing computing time. Below, we look at different ways of parallelization. 9.1.2 Embarrassingly Parallel Scatter-Gather A sub-task is called “Embarrassingly Parallel” if the sub-task can be run completely independent of other sub-tasks to finish the task. When we run multiple samples in our workflow, it is a form of “Embarrassingly Parallel”, because processing each tumor-normal pair through the workflow is done independently of other samples on separate computers (also known in high performance computing as “nodes”). This technique reduces the computing time and breaks down CPU and memory usage into smaller, more affordable resource requests. Later in this chapter we will show an detailed example of using this technique to split a BAM into individual chromosomes to run a resource demanding task. 9.1.3 Multithreading (Shared-Memory Parallelism) Sometimes, you want to use multiple CPUs on a single computer to run sub-components of your task in parallel. This is called “multithreading”. Here, a single task accesses multiple CPUs on a computer to split up the task, and these CPUs share the same memory. Due to shared memory, each of these CPUs are not quite independent, and some communication is needed between CPUs to make this efficient. Well-documented and optimized bioinformatics software will often have usage options to use multiple cores to speed up the task. In our previous chapters, we used BWA MEM with an optional default of 16 CPUs. If you are developing your own task, then there are many built in operations that can make use of multiple CPUs, as well as low-level threading methods to help you develop a multi-threaded program. 9.1.4 Multiprocessing (Distributed-Memory Parallelism) Sometimes, the scale of the work requires coordination of multiple computers working dependently of each other. This is called “multiprocessing”. This is common in working with large scale data, using tools such as Spark or Hadoop. We will not be talking about multiprocessing extensively in this guide. 9.1.5 Graphical Processing Units (GPUs) Graphical processing units are an additional processing tool to CPUs, in which a different computer hardware is used. There are usually a few handful of CPUs on a computer, but there can be thousands of GPUs, each capable of doing a small and simple task quickly. GPUs are used for graphical displays, and scientific computing problems such as training neutral networks and solving differential equations. We will not be talking about GPUs extensively in this guide. 9.2 Scatter-Gather on chromosomes Scatter-Gather By Chromosome Suppose that one our tasks, ApplyBaseRecalibrator, is taking too long to run and taking on too much memory. We might first look into the tool’s documentation to see if there are ways use it more optimally, such as provide more CPUs, but we don’t see it. We turn to Embarrassingly Parallel Scatter-Gather, in which we split a BAM file by its chromosomes to create 23 BAM files, run ApplyBaseRecalibrator on each of these BAMs, and merge all of these calibrated BAM files together. This is called “Scatter-Gather” because we have to scatter our BAMs into smaller parts to run, and then gather them together in the end. Split By Chromosome We first write a new task to split our BAM file, which takes the original BAM, an array of chromosomes to scatter, and outputs an array of Files of the smaller BAM files. The output array of Files is stored as bams and indexFiles, and illustrated in the diagram as “chr_array”. # Split a BAM file by chromosomes task splitBamByChr { input { File bamToSplit File baiToSplit Array[String] chromosomes } String baseFileName = basename(bamToSplit, &quot;.bam&quot;) command &lt;&lt;&lt; set -eo pipefail #For each chromosome... for x in ~{sep=&#39; &#39; chromosomes}; do outputFile=&quot;~{baseFileName}_${x}.bam&quot; samtools view -b -@ 3 &quot;~{bamToSplit}&quot; $x &gt; $outputFile samtools index $outputFile done # List all bam and bai files created ls *.bam &gt; bam_list.txt ls *.bam.bai &gt; bai_list.txt &gt;&gt;&gt; output { Array[File] bams = read_lines(&quot;bam_list.txt&quot;) Array[File] indexFiles = read_lines(&quot;bai_list.txt&quot;) } runtime { docker: &quot;fredhutch/bwa:0.7.17&quot; cpu: 4 } GatherBams We also need a task to gather all the BAMs together, which takes in an array of Files, and outputs a merged BAM. The input array of Files is referred as bams, and illustrated in the diagram as chr_array. #Gather an array of BAMs task gatherBams { input { Array[File] bams String clean_baseName_regex } String temp = basename(bams[0], &quot;.bam&quot;) String baseFileName = sub(temp, clean_baseName_regex, &quot;&quot;) command &lt;&lt;&lt; set -eo pipefail samtools merge -c -@3 &quot;~{baseFileName}&quot;.merged.bam ~{sep=&#39; &#39; bams} samtools index &quot;~{baseFileName}&quot;.merged.bam &gt;&gt;&gt; runtime { cpu: 4 docker: &quot;fredhutch/bwa:0.7.17&quot; } output { File merged_bam = &quot;~{baseFileName}.merged.bam&quot; File merged_bai = &quot;~{baseFileName}.merged.bam.bai&quot; } } In our workflow, after MarkDuplicates task, we first use splitBamByChr to divvy up our BAM file, and then use a scatter over the output array of scattered BAMs to run ApplyBaseRecalibrator on each of the scatted BAMs. Finally, we take the output of ApplyBaseRecalibrator as an array to be merged together via gatherBams: Scatter Gather Full Workflow #Split by chromosomes call splitBamByChr as tumorSplitBamByChr { input: bamToSplit = tumorMarkDuplicates.markDuplicates_bam, baiToSplit = tumorMarkDuplicates.markDuplicates_bai, chromosomes = chrs_to_split } #Scatter by chromosomes scatter(i in range(length(tumorSplitBamByChr.indexFiles))) { File tumorSubBam = tumorSplitBamByChr.bams[i] File tumorSubBamIndex = tumorSplitBamByChr.indexFiles[i] call ApplyBaseRecalibrator as tumorApplyBaseRecalibrator { input: input_bam = tumorSubBam, input_bam_index = tumorSubBamIndex, dbSNP_vcf = dbSNP_vcf, dbSNP_vcf_index = dbSNP_vcf_index, known_indels_sites_VCFs = known_indels_sites_VCFs, known_indels_sites_indices = known_indels_sites_indices, refGenome = refGenome } } #Gather all chromosomes together call gatherBams as tumorGatherBams { input: bams = tumorApplyBaseRecalibrator.recalibrated_bam, clean_baseName_regex = &quot;.duplicates_marked_12.recal&quot; } Our final workflow: Loading… "],["appendix-backends-and-executors.html", "Chapter 10 Appendix: Backends and Executors 10.1 Commonly used runtime attributes 10.2 General advice 10.3 Executor-specific notes 10.4 Backend-specific notes", " Chapter 10 Appendix: Backends and Executors Generally speaking, WDL workflows are quite portable thanks to their usage of Docker images to maintain software dependencies. However, the executor used to run WDLs and what backend they are being run upon can lead to specific scenarios where minor tweaks to your WDL are necessary to ensure portability. 10.1 Commonly used runtime attributes Runtime attributes do not behave the same on all platforms. Here are how some of the most commonly used runtime attributes work on some of the most common WDL setups. Attribute Fred Hutch HPC Local Cromwell Local miniwdl Terra bootDiskSizeGB n/a n/a n/a Request a disk of this size to boot the Docker image (useful for very large Docker images) cpu Reserve at most this many cores n/a Reserve at most this many cores Request a minimum of this many cores (scales with memory) disks n/a n/a n/a Request this much disk size - soft requirement, if not specified, will request 10 GB docker Run the task in this Docker image Run the task in this Docker image Run the task in this Docker image Run the task in this Docker image memory n/a n/a Maximum amount of memory to use Minimum amount of memory to use (scales with CPU) preemptible n/a n/a n/a Attempt running on a preemptible instance this many times, then switch to a non-preemptible walltime, partition, modules, dockerSL, account See back-end notes for Fred Hutch HPC below. n/a n/a n/a 10.2 General advice Consider using more runtime attributes, not fewer. miniwdl and Cromwell will ignore runtime attributes as necessary, so including a runtime attribute that only applies to a particular backend will not harm portability on other backends. The Dockstore CLI, as of v1.15, uses Cromwell to run WDLs. Advice about Cromwell will therefore also apply to the Dockstore CLI. If running a workflow with a scattered task on a local compute, consider using miniwdl instead of Cromwell. Whether you are using miniwdl or Cromwell locally, make sure Docker has enough resources to be able to download and run the Docker images specified in your WDL tasks. Workflow systems with a UI like Terra may become unresponsive if you run a task scattered more than 1000x. Outputs may need to be interacted with using an API specific to that backend. 10.3 Executor-specific notes 10.3.1 Cromwell Cromwell running on a local machine has a tendency to heavily use system resources, especially when running scattered tasks. Sometimes, it will use too many resources at once. When this happens, your workflow’s tasks will tend to fail with exit code 137, or you will see hints about running out of memory in the logs of your tasks. You may also observe Docker becoming unresponsive. You can fix Docker by restarting Docker or your machine, but you will likely want to prevent this issue rather than keep having to restart Docker. To prevent this, modify concurrent-job-limit for your backend in the Cromwell configuration file. The default Cromwell configuration file,, can be found on Cromwell’s GitHub repo. To point Cromwell to a specific configuration file, run Cromwell with the flag -Dconfig.file= followed by the address of the config file to use. If you are using the Dockstore CLI, follow these instructions to use a custom configuration file for Dockstore’s version of Cromwell. Because the Docker lockup issue mentioned previously isn’t uncommon, it is recommended to use a custom configuration file that limits concurrent-job-limit process on a local backend. Other notes: Cromwell runs as a jar file, so it is very portable and does not need to be “installed” provided you have a modern Java runtime environment. If you just want to check workflows are valid, you can run womtool as a jar file (available on Cromwell’s GitHub page). This will check not only the WDL file you pass it, but also any WDLs it imports. Cromwell does not use call caching on most backends, but it is the default on Terra. For non-Terra backends, it can be enabled in the Cromwell configuration file. Cromwell supports the gpu and disks runtime attributes on certain backends. If using the gpu runtime attribute, make sure your task is set up correctly to properly use this resource. Generally speaking, Cromwell will be able to directly modify input files by default without causing permission errors, unlike miniwdl. Cromwell uses a system known as hog factors to adjust how task order is prioritized. 10.3.2 miniwdl The authors of this course have informally observed that miniwdl appears to be “safer” in terms of using up too many resources than Cromwell. However, if you are sharing a compute with other users, it is still worth looking at the miniwdl configuration file and limiting how many Docker images you spin up at the same time. The default miniwdl configuration file, which you can use as a template for setting up your own configuration file for miniwdl, can be found on miniwdl’s GitHub repo. To point miniwdl to a specific configuration file, run miniwdl run with the --cfg flag. Other notes: miniwdl is a Python program, and it must be installed via pip or pip3 to use. miniwdl’s equivalent to womtool is miniwdl check, but it also includes shellcheck to check the command section of your tasks. miniwdl supports call caching, but it is turned off by default. miniwdl does not support the gpu or disks runtime attributes and will ignore them if present in a task’s runtime section. By default, miniwdl does not duplicate input files. If your workflow only needs to read input files, this helps save disk space, but if your workflow directly modifies input files, this can result in permission errors. A simple fix is to run miniwdl with the --copy-input-files flag. 10.4 Backend-specific notes 10.4.1 HPCs It is difficult to provide specific advice on HPCs, as they can vary greatly. Some general notes: Some HPCs do not support the use of Docker due to security concerns HPCs that do not allow the use of Docker may be able to run WDLs using alternative container technologies such as podman or rootless Docker Some HPCs will use disks to determine which disk to run on, which can be useful for managing disk space 10.4.1.1 Fred Hutch HPC PROOF is a Shiny frontend that launches Cromwell Server and lets you manage your jobs from a GUI. User guide can be found here. The Fred Hutch HPC supports the use of multiple JSON files going into the same workflow The Fred Hutch HPC supports the use of Docker, so the docker runtime attribute works as expected memory and gpu runtime attributes are ignored, but cpu works as expected. Here is some information on approximating memory use. walltime attribute is a string (“HH:MM:SS”) that specifies how much time you want to request for the task. Can also specify &gt;1 day, e.g. “1-12:00:00” is 1 day+12 hours. The amount of walltime you can request depends on which cluster partition you want to use. More info here. partition attribute specifies the cluster partitionto use. The default is “campus-new”. modules attribute is a a space-separated list of the environment modules you’d like to load (in that order) prior to running the task. For example, “modules:”GATK/4.2.6.1-GCCcore-11.2.0 SAMtools/1.16.1-GCC-11.2.0”, the GATK module will be loaded first, followed by the SAMtools module. account attribute allows users connected to multiple PI accounts to specify which account to use for each task, to manage cluster allocations. An example of the value for this variable is “paguirigan_a”, following the PI’s “lastname_firstNameInitital” pattern. 10.4.2 GCP/Terra GCP powers Terra, which is a Cromwell-based platform that is commonly used for running WDL workflows, but it can also be used on its own. Some special considerations need to be taken into account when designing workflows for compatibility on this backend. 10.4.2.1 Preemptibles Preemptible machines are an excellent way to save money when running workflows. A preemptible machine is a Google Cloud machine that is significantly cheaper (often less than half the price) than a standard one, at the cost of potentially being stopped suddenly. When running a task on a preemptible machine using Cromwell, if the preemptible is preempted (stopped suddenly), Cromwell will automatically retry the task. This does mean that in a worst case scenario, such a task could take about twice as long to run as normal and end up slightly more expensive, so you will want to weigh the costs and benefits. As a general rule of thumb, if you expect a task to take less than 2 hours, it is usually worth trying to use preemptible machines. 10.4.2.2 Disk space Running Cromwell on GCP is one of the few times that the disks runtime attribute is a soft requirement. Due to how Cromwell works, it must request a certain amount of disk space from GCP before running the task’s command section. In other words, you must know roughly how much disk space your task will use before it runs. If not specified in the runtime attribute of a task, Cromwell will request 10 GB of SSD space. A helpful way to handle disk space in WDLs you anticipate will be run on GCP/Terra is to make disk size a function of the size of your task inputs. The task inputs and any private variable declarations made outside of the command section are all calculated before Cromwell requests a disk from GCP, so you can use this section to define a disk size function. For example: version 1.0 task do_something { input { Array[File] some_array_of_files File one_file } Int predicted_disk_space = ceil(size(some_array_of_files, &quot;GB&quot;)) + ceil(size(one_file, &quot;GB&quot;)) + 1 # size(x, &quot;GB&quot;) returns a float representing the size of x in gigabytes # ceil() is a WDL built-in function that rounds a float up into an integer command &lt;&lt;&lt; pass &gt;&gt;&gt; runtime { docker: &quot;ubuntu-latest&quot; disks: &quot;local-disk &quot; + predicted_disk_space + &quot; HDD&quot; } } Common pitfalls: Not wrapping size() in ceil() – the disks: runtime attribute requires an integer, not a float Not using \"GB\" when calling size() – if you don’t specify units for size() it will return bytes! Forgetting the space between “local-disk” and the integer (or the integer and “HDD”/“SSD”) As a more in-depth example of how to use disk space, see the following WDL: version 1.0 task autoscale_disk_size_for_basically_anything { input { File ref_genome # required file File? ref_alt # optional file Array[File] fastqs # required array of files File tarball_bams # uncompressed tarball of bam files File compressed_vcfs # compressed (tar.gz) tarball of VCFs Int? addl_disk_gb # user-defined integer } # ref_genome always exists, so we can size() on it safely. size() returns a float, # but some backends require disk size is an integer, so we use ceil() to turn # that float into an integer. ceil() always rounds up. # You should ALWAYS explictly defining which unit you are using with size(). I # recommend sticking with GB. If you do not define a unit, it is easy to accidentally # request ludricious amounts of disk space, eg: # 1 GB --&gt; 1073741824 bytes --&gt; 1073741824 GB --&gt; 1.07 exabytes Int disk_ref = ceil(size(ref_genome, &quot;GB&quot;)) # ref_alt does not always exist, and size() will break if you try to use it on an # undefined file. To prevent this, we use select_first to fall back on the always- # defined ref_genome file if ref_alt is undefined. Int disk_alt = ceil(size(select_first([ref_alt, ref_genome]), &quot;GB&quot;)) # Of course, this means that if ref_alt is undefined, then we don&#39;t really need # disk_alt, because the size of ref_genome is already covered by disk_ref. Int disk_both_refs = if defined(ref_alt) then disk_alt + disk_ref else disk_ref # You might be wondering why we don&#39;t just do this: # Int disk_alt = if defined(ref_alt) then ceil(size(ref_alt, &quot;GB&quot;)) else 0 # This is synatically valid, but generally speaking it is best to avoid running # WDL built-in functions (except select_all, select_first, and defined) on optional # variables as much as possible. # When dealing with arrays, size() will return the sum of all Files in the array. Int disk_fastqs = ceil(size(fastqs, &quot;GB&quot;)) # If you are passing in a tarball, you will likely be expanding it. If so, consider # doubling the disk size to account for it in both states. Int disk_tarball = 2*ceil(size(tarball_bams, &quot;GB&quot;)) # A compressed tar.gz is harder to predict. As a very rough rule of thumb, 10x should # be enough. If you are passing in a tar.gz made from an upstream task, you may want # to decrease this number based on the compression ratio you used. Int disk_compressed = 10*ceil(size(compressed_vcfs)) # Because addl_disk_gb is an optional integer with no fallback, you cannot add it to # another integer, eg, this will error as &quot;Non-numeric operand to + operator&quot;: # Int final_disk_size = disk_both_refs + disk_fastqs + disk_tarball + disk_compressed + addl_disk_gb # We have to use select_first() to coerce it into a not-optional integer first. Int disk_addl = select_first([addl_disk_gb, 0]) # Finally, let&#39;s add everything up. (Recall that disk_both_refs accounts for ref_genome and # also ref_alt.) Int final_disk_size = disk_both_refs + disk_fastqs + disk_tarball + disk_compressed + disk_addl command &lt;&lt;&lt; echo disk_ref: ~{disk_ref} echo disk_alt: ~{disk_alt} echo disk_both_refs: ~{disk_both_refs} echo disk_fastqs: ~{disk_fastqs} echo disk_tarball: ~{disk_tarball} echo disk_compressed: ~{disk_compressed} echo addl_disk_gb: ~{addl_disk_gb} echo final_disk_size: ~{final_disk_size} &gt;&gt;&gt; runtime { # On GCP!Cromwell, if you do not define the units in the runtime attribute, this # will be interpreted as the size in GB. This is why it&#39;s so easy to accidentally # request the wrong amount of disk space if you do not define the units for size(); # In one place WDL defaults to bytes but in the other it defaults to GB. disks: &quot;local-disk &quot; + final_disk_size + &quot; HDD&quot; } } Loading… "],["about-the-authors.html", "About the Authors", " About the Authors These credits are based on our course contributors table guidelines.     Credits Names Lead Content Instructor(s) Ash O’Farrell, Sitapriya Moorthi, Chris Lo Content Editor(s)/Reviewer(s) Amy Paguirigan, Carrie Wright, Ted Laderas Content Director(s) Amy Paguirigan Content Consultants Ash O’Farrell Production Content Publisher(s) Carrie Wright Technical Template Publishing Engineers Candace Savonen, Carrie Wright, Ava Hoffman Publishing Maintenance Engineer Candace Savonen Technical Publishing Stylists Carrie Wright, Ava Hoffman, Candace Savonen Package Developers (ottrpal) Candace Savonen, John Muschelli, Carrie Wright   ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.3.2 (2023-10-31) ## os Ubuntu 22.04.4 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Etc/UTC ## date 2025-01-10 ## pandoc 3.1.1 @ /usr/local/bin/ (via rmarkdown) ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date (UTC) lib source ## bookdown 0.41 2024-10-16 [1] CRAN (R 4.3.2) ## bslib 0.6.1 2023-11-28 [1] RSPM (R 4.3.0) ## cachem 1.0.8 2023-05-01 [1] RSPM (R 4.3.0) ## cli 3.6.2 2023-12-11 [1] RSPM (R 4.3.0) ## devtools 2.4.5 2022-10-11 [1] RSPM (R 4.3.0) ## digest 0.6.34 2024-01-11 [1] RSPM (R 4.3.0) ## ellipsis 0.3.2 2021-04-29 [1] RSPM (R 4.3.0) ## evaluate 0.23 2023-11-01 [1] RSPM (R 4.3.0) ## fastmap 1.1.1 2023-02-24 [1] RSPM (R 4.3.0) ## fs 1.6.3 2023-07-20 [1] RSPM (R 4.3.0) ## glue 1.7.0 2024-01-09 [1] RSPM (R 4.3.0) ## htmltools 0.5.7 2023-11-03 [1] RSPM (R 4.3.0) ## htmlwidgets 1.6.4 2023-12-06 [1] RSPM (R 4.3.0) ## httpuv 1.6.14 2024-01-26 [1] RSPM (R 4.3.0) ## jquerylib 0.1.4 2021-04-26 [1] RSPM (R 4.3.0) ## jsonlite 1.8.8 2023-12-04 [1] RSPM (R 4.3.0) ## knitr 1.48 2024-07-07 [1] CRAN (R 4.3.2) ## later 1.3.2 2023-12-06 [1] RSPM (R 4.3.0) ## lifecycle 1.0.4 2023-11-07 [1] RSPM (R 4.3.0) ## magrittr 2.0.3 2022-03-30 [1] RSPM (R 4.3.0) ## memoise 2.0.1 2021-11-26 [1] RSPM (R 4.3.0) ## mime 0.12 2021-09-28 [1] RSPM (R 4.3.0) ## miniUI 0.1.1.1 2018-05-18 [1] RSPM (R 4.3.0) ## pkgbuild 1.4.3 2023-12-10 [1] RSPM (R 4.3.0) ## pkgload 1.3.4 2024-01-16 [1] RSPM (R 4.3.0) ## profvis 0.3.8 2023-05-02 [1] RSPM (R 4.3.0) ## promises 1.2.1 2023-08-10 [1] RSPM (R 4.3.0) ## purrr 1.0.2 2023-08-10 [1] RSPM (R 4.3.0) ## R6 2.5.1 2021-08-19 [1] RSPM (R 4.3.0) ## Rcpp 1.0.12 2024-01-09 [1] RSPM (R 4.3.0) ## remotes 2.4.2.1 2023-07-18 [1] RSPM (R 4.3.0) ## rlang 1.1.4 2024-06-04 [1] CRAN (R 4.3.2) ## rmarkdown 2.25 2023-09-18 [1] RSPM (R 4.3.0) ## sass 0.4.8 2023-12-06 [1] RSPM (R 4.3.0) ## sessioninfo 1.2.2 2021-12-06 [1] RSPM (R 4.3.0) ## shiny 1.8.0 2023-11-17 [1] RSPM (R 4.3.0) ## stringi 1.8.3 2023-12-11 [1] RSPM (R 4.3.0) ## stringr 1.5.1 2023-11-14 [1] RSPM (R 4.3.0) ## urlchecker 1.0.1 2021-11-30 [1] RSPM (R 4.3.0) ## usethis 2.2.3 2024-02-19 [1] RSPM (R 4.3.0) ## vctrs 0.6.5 2023-12-01 [1] RSPM (R 4.3.0) ## xfun 0.48 2024-10-03 [1] CRAN (R 4.3.2) ## xtable 1.8-4 2019-04-21 [1] RSPM (R 4.3.0) ## yaml 2.3.8 2023-12-11 [1] RSPM (R 4.3.0) ## ## [1] /usr/local/lib/R/site-library ## [2] /usr/local/lib/R/library ## ## ────────────────────────────────────────────────────────────────────────────── "],["references.html", "Chapter 11 References", " Chapter 11 References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

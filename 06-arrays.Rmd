```{r, include = FALSE}
ottrpal::set_knitr_image_path()
```

# Using Arrays For Parallelization and Other Use Cases

We have a workflow that runs on a single sample. What if we want to process multiple samples at once? Let's look at the various ways we can run our workflow more efficiently, as well as processing many samples in parallel. This is where WDL really shines.

- How to use scattered tasks to run a workflow on multiple samples at once

- How to use arrays effectively

- How to reference arrays in a task's command section

- How arrays differ from Structs

## The array type
Arrays are essentially lists of another [primative type](https://en.wikipedia.org/wiki/Primitive_data_type). It is most common to see Array[File] in WDLs, but an array can contain integers, floats, strings, and the like. An array can only have one of a given primative type. For example, an Array[String] could contain the strings "cat" and "dog" but not the integer 1965 (however, it could have "1965" as a string).

## Referencing an array in a task

If the input variable is an array, we must include an array separator. In WDL 1.0, this is done using the `sep=` expression placeholder. Every value in the WDL Array[String] will be separated by whatever value is declared via `sep`. In this example, that is a simple space, as that is one way how to construct a bash variable.

```
task count_words {
  input {
    Array[String] a_big_sentence
  }
  command <<<
    ARRAY_OF_WORDS=(~{sep=" " a_big_sentence})
    echo ${#ARRAY_OF_FILES[@]} >> length.txt
    # Note how the bash array uses ${} syntax, which could quickly get
    # confusing if we used that syntax for our WDL variables. This is
    # why we recommend using tilde + {} for your WDL variables.
  >>>
}
```
It's usually unnecessary to declare an Array[String], because a single String can have many words in it. That being said, an Array[String] can sometimes come in handy if it is made up of outputs from other tasks. We'll talk more about chaining tasks together in upcoming chapters.

::: {.notice data-latex="warning"}
The WDL 1.1 spec added a new built-in function, `sep()`, which replaces the `sep=` expression placeholder for arrays. This same version of the spec also notes that the `sep=` expression placeholder [are deprecated and will be removed from future versions of WDL](https://github.com/openwdl/wdl/blob/main/versions/1.1/SPEC.md#-expression-placeholder-options). For the time being, we recommend sticking with `sep=` as it is compatible with both WDL 1.0 and WDL 1.1, even though it is technically deprecated in WDL 1.1.
:::

If you're not used to working in bash, the syntax for interacting with bash arrays can be unintuitive, but you don't have to write a WDL's command section only using bash. In fact, working in another language besides bash within a WDL can be a great way to write code quickly, or perform tasks that are more advanced than what a typical bash script can handle. Just be sure to set `sep` properly to ensure that your array is interpreted correctly. In this example, we place quotation marks before and after the variable to ensure that the first and last value of the list are given beginning and ending quotation marks respectively.

```
task count_words_python {
  input {
    Array[String] a_big_sentence
  }
  command <<<
    python << CODE
    sentence = [ "~{sep='", "' a_big_sentence}" ]
    print(len(sentence))
    CODE
  >>>
  runtime {
    docker: "python:latest"
  }
}
```


## Scattered tasks
Scattered tasks allow us to run a WDL task in parallel. This is especially useful on highly scalable backends such as HPCs or the cloud, as it allows us to potentially run hundreds or even thousands of instances of a task at the same time. The most common use case for this is processing many samples at the same time, but it can also be used for processing a single sample's chromosomes in parallel, or similiar situations where breaking up data into discrete "chunks" makes sense.

It should be noted that a scattered task does not work the same way as multithreading, nor does it correlate with the `cpu` WDL runtime attribute. Every instance of a scattered task takes place in a new Docker image, and is essentially "unaware" of all other instances of that scattered task, with one exception: If an instance of a scattered task errors out, a WDL executor may attempt to shut down other ongoing instances of that scattered task.

### Troubleshooting
Scattered tasks are relatively simple in theory, but the way they interact with optional types can be unintutive. If you are running into issues running scattered tasks on optional types, please see the section on optional types. As a general rule, you should avoid using optional types as the input of a scattered task whenever possible.

Generally speaking, a WDL executor will try to run as many instances of a scattered task as it thinks your backend's hardware can handle at once. Sometimes the WDL executor will overestimate what the backend is capable of and run too many instances of a scattered task at once. This almost never happens on scaleable cloud-based backends such as Terra, but isn't uncommon when running scattered tasks on a local machine. For advice on diagnosising this issue and how to prevent it, please see the optimization chapter of this course.

## Making our workflow run on multiple samples at once using scattered tasks and arrays
When we originally wrote our workflow, we designed it to only run on a single sample at a time. However, we'll likely want to run this workflow on multiple samples at the same time. For some workflows, this is a great way to directly compare samples to each other, but for our purposes we simply want to avoid running a workflow 100 times if we can instead run one workflow that handles 100 samples at once.

For starters, we'll want to change our tumor and normal samples from File to Array[File].





## Nested arrays: Splitting by chromosome


## Nested arrays: Splitting by forward and reverse read
Our workflow assumes that every sample has a single fastq file, such as `SRR8618962.fastq`. However, fastq files are sometimes split by read direction, resulting in `SRR8618962_1.fastq` and `SRR8618962_2.fastq`. How would we modify our workflow to accept this sort of input, and still be able to run on multiple samples at once?

The first thing to keep in mind is that we no longer represent a single sample as a single `File sampleFastq`, but instead as `Array[File] sampleFastqs`. In real life, fastq files may be split even further than `_1` and `_2`, such as by which lane of an Illumina cell they were sequenced in. The same concept we will go over here can be applied to multilane samples too, but for now we will focus on the `_1` and `_2` case for simplicity. This means that each sample is an array with precisely two values.

Our first step in the workflow is bwa mem, which changes a single sample's fastq file(s) into a single bam file. No matter how many fastqs we input for a single sample, we will always end up with just one bam file to represent that sample. That means that, for this workflow in particular, we only need to change the first task.


TODO: edits of bwa call


Modifying our workflow to run on multiple samples requires the use of nested arrays. Our workflow-level input `Array[File] sampleFastqs` needs to become `Array[Array[File]] allSampleFastqs`. Each internal array represents a pair of fastqs, which we could enter via JSON like so:

```
"minidata_mutation_calling_v1.allSampleFastqs": [
    ["/fh/scratch/delete90/_DaSL/WDL_mini_data/NCIH2172_LUNG/SRR8618962_1.fastq", "/fh/scratch/delete90/_DaSL/WDL_mini_data/NCIH2172_LUNG/SRR8618962_2.fastq"],
    ["/fh/scratch/delete90/_DaSL/WDL_mini_data/NA12878_Broad/H06JUADXX130110.1.ATCACGAT.20k_interleaved_1.fastq", "/fh/scratch/delete90/_DaSL/WDL_mini_data/NA12878_Broad/H06JUADXX130110.1.ATCACGAT.20k_interleaved_2.fastq"]]
```
